[2024-04-26T12:45:13.840+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-04-26T12:45:12.116390+00:00 [queued]>
[2024-04-26T12:45:13.844+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-04-26T12:45:12.116390+00:00 [queued]>
[2024-04-26T12:45:13.845+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-04-26T12:45:13.849+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-04-26 12:45:12.116390+00:00
[2024-04-26T12:45:13.853+0000] {standard_task_runner.py:57} INFO - Started process 53 to run task
[2024-04-26T12:45:13.864+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'manual__2024-04-26T12:45:12.116390+00:00', '--job-id', '14', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmp7bt9kvgl']
[2024-04-26T12:45:13.875+0000] {standard_task_runner.py:85} INFO - Job 14: Subtask reddit_extraction
[2024-04-26T12:45:13.933+0000] {task_command.py:416} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-04-26T12:45:12.116390+00:00 [running]> on host 1bd1b115cb8b
[2024-04-26T12:45:13.988+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Aida Sow' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-04-26T12:45:12.116390+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-04-26T12:45:12.116390+00:00'
[2024-04-26T12:45:13.997+0000] {logging_mixin.py:151} INFO - connected to reddit!
[2024-04-26T12:45:14.906+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I currently work at a large firm with a very large sap erp enterprise instance. Over the past two years, I\'ve encountered more issues with SAP product teams and consultants than with any technology in my entire career prior. \n\n SAP is such a shitty company; it\'s just disgusting. Lately, they disallowed the use of ODP RFC replication services, which basically outlawed any integration tool that uses this method to replicate SAP data to the cloud, e.g., Qlik, Azure Data Factory, Talend, and many more. \n\nIt’s no coincidence that this change coincided with the launch of their "new" rebranded data warehouse, Data Sphere, where the costs of moving data into cloud services are exorbitant. Additionally, they\'ve deliberately limited access to Data Sphere via their oData API for replication services. \n\nI know this is basically a rant but the amount of bullshit is just baffling. How do you guys deal with the SAP virus and what its your funny story?', 'author_fullname': 't2_793tedn0', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Whats your horror story with SAP Integration?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccv2r3', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 122, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 122, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714059468.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I currently work at a large firm with a very large sap erp enterprise instance. Over the past two years, I&#39;ve encountered more issues with SAP product teams and consultants than with any technology in my entire career prior. </p>\n\n<p>SAP is such a shitty company; it&#39;s just disgusting. Lately, they disallowed the use of ODP RFC replication services, which basically outlawed any integration tool that uses this method to replicate SAP data to the cloud, e.g., Qlik, Azure Data Factory, Talend, and many more. </p>\n\n<p>It’s no coincidence that this change coincided with the launch of their &quot;new&quot; rebranded data warehouse, Data Sphere, where the costs of moving data into cloud services are exorbitant. Additionally, they&#39;ve deliberately limited access to Data Sphere via their oData API for replication services. </p>\n\n<p>I know this is basically a rant but the amount of bullshit is just baffling. How do you guys deal with the SAP virus and what its your funny story?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ccv2r3', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Ok-Sentence-8542'), 'discussion_type': None, 'num_comments': 74, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccv2r3/whats_your_horror_story_with_sap_integration/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccv2r3/whats_your_horror_story_with_sap_integration/', 'subreddit_subscribers': 179075, 'created_utc': 1714059468.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.908+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Credit to Julien H for sharing this on Linkedin. Kinda didn't believe it at first but there you go\n\n&#x200B;\n\nhttps://preview.redd.it/j62ujlacrmwc1.png?width=1884&format=png&auto=webp&s=4f324f6092a8dab48f6bc919cc13d8bcd5b10b57\n\nLooks like people are realising our importance for Gen AI slowly, but surely.", 'author_fullname': 't2_voma7dkju', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'The Data Engineering Hype Cycle is beginning (??)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 80, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'j62ujlacrmwc1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 62, 'x': 108, 'u': 'https://preview.redd.it/j62ujlacrmwc1.png?width=108&crop=smart&auto=webp&s=a33009471e410d729582e2123a93c8f97644d24e'}, {'y': 124, 'x': 216, 'u': 'https://preview.redd.it/j62ujlacrmwc1.png?width=216&crop=smart&auto=webp&s=831e33254d5d74365b686e1b9de469bdd791ea51'}, {'y': 184, 'x': 320, 'u': 'https://preview.redd.it/j62ujlacrmwc1.png?width=320&crop=smart&auto=webp&s=4358aab93c75e7cd9902cb82195534fcd6a58b8b'}, {'y': 369, 'x': 640, 'u': 'https://preview.redd.it/j62ujlacrmwc1.png?width=640&crop=smart&auto=webp&s=dcef815c0687285cf7f2795f483e007c338d756c'}, {'y': 553, 'x': 960, 'u': 'https://preview.redd.it/j62ujlacrmwc1.png?width=960&crop=smart&auto=webp&s=df56f37e3f1e2f472c668bebcf8b958ecce723b3'}, {'y': 623, 'x': 1080, 'u': 'https://preview.redd.it/j62ujlacrmwc1.png?width=1080&crop=smart&auto=webp&s=64832b1cd72507d4368177bd5621773e310a2347'}], 's': {'y': 1087, 'x': 1884, 'u': 'https://preview.redd.it/j62ujlacrmwc1.png?width=1884&format=png&auto=webp&s=4f324f6092a8dab48f6bc919cc13d8bcd5b10b57'}, 'id': 'j62ujlacrmwc1'}}, 'name': 't3_1ccsf39', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 116, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 116, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/KtNG9MvmMTzrlRRV0gGLYGSh3orAAmS-Q7L79zfkOcU.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714052953.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Credit to Julien H for sharing this on Linkedin. Kinda didn&#39;t believe it at first but there you go</p>\n\n<p>&#x200B;</p>\n\n<p><a href="https://preview.redd.it/j62ujlacrmwc1.png?width=1884&amp;format=png&amp;auto=webp&amp;s=4f324f6092a8dab48f6bc919cc13d8bcd5b10b57">https://preview.redd.it/j62ujlacrmwc1.png?width=1884&amp;format=png&amp;auto=webp&amp;s=4f324f6092a8dab48f6bc919cc13d8bcd5b10b57</a></p>\n\n<p>Looks like people are realising our importance for Gen AI slowly, but surely.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ccsf39', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='engineer_of-sorts'), 'discussion_type': None, 'num_comments': 62, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccsf39/the_data_engineering_hype_cycle_is_beginning/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccsf39/the_data_engineering_hype_cycle_is_beginning/', 'subreddit_subscribers': 179075, 'created_utc': 1714052953.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.908+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I've been an avid user of Dagster and feel pretty bought into the move towards declarative orchestration with an asset-first model, but I've also been watching what has been going on with other players in the space, with Airflow (open source), Astronomer, Prefect, Mage, Kestra, etc. \n\nPrefect had a ton of momentum a couple of years ago, but you hear about it less and less, and you see more and more about Mage (but it looks like its more for IC data engineers), and Astronomer has been making strides to address the weak points of Airflow. \n\nWhat is everyone using? Where do you see this technology space going? ", 'author_fullname': 't2_1bvt2z', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What is the state of the data orchestration market?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccueqx', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.95, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 39, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': '620fb7b8-ac9d-11eb-a99a-0ed5d8300de1', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 39, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714057850.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;ve been an avid user of Dagster and feel pretty bought into the move towards declarative orchestration with an asset-first model, but I&#39;ve also been watching what has been going on with other players in the space, with Airflow (open source), Astronomer, Prefect, Mage, Kestra, etc. </p>\n\n<p>Prefect had a ton of momentum a couple of years ago, but you hear about it less and less, and you see more and more about Mage (but it looks like its more for IC data engineers), and Astronomer has been making strides to address the weak points of Airflow. </p>\n\n<p>What is everyone using? Where do you see this technology space going? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Tech Lead', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ccueqx', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Similar_Estimate2160'), 'discussion_type': None, 'num_comments': 17, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1ccueqx/what_is_the_state_of_the_data_orchestration_market/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccueqx/what_is_the_state_of_the_data_orchestration_market/', 'subreddit_subscribers': 179075, 'created_utc': 1714057850.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.909+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I've written a Python script that pulls data from a JSON API. I would like to automate this so the data gets pulled every day and stored in a SQL database.\n\nAbout 5,000 rows every day.\n\nWhat would be the easiest and cheapest approach to handle this?", 'author_fullname': 't2_142f6qp6', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Easiest and cheapest approach for daily fetch of data', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cd0tmv', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 20, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 20, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714074741.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;ve written a Python script that pulls data from a JSON API. I would like to automate this so the data gets pulled every day and stored in a SQL database.</p>\n\n<p>About 5,000 rows every day.</p>\n\n<p>What would be the easiest and cheapest approach to handle this?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cd0tmv', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='LouisDeconinck'), 'discussion_type': None, 'num_comments': 21, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cd0tmv/easiest_and_cheapest_approach_for_daily_fetch_of/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cd0tmv/easiest_and_cheapest_approach_for_daily_fetch_of/', 'subreddit_subscribers': 179075, 'created_utc': 1714074741.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.910+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Preferred ways of transforming your JSON data, preferred tools for querying JSON, etc.', 'author_fullname': 't2_lnwagoki', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Tips on Dealing with JSON Data', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccsms9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 17, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 17, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714053512.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Preferred ways of transforming your JSON data, preferred tools for querying JSON, etc.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ccsms9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='AMDataLake'), 'discussion_type': None, 'num_comments': 21, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccsms9/tips_on_dealing_with_json_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccsms9/tips_on_dealing_with_json_data/', 'subreddit_subscribers': 179075, 'created_utc': 1714053512.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.910+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I wanna improve my programming skills and to optimize loading large files of data in Pyspark but I feel if you dont work in a company with Databricks, big paid clusters... it's so hard to practice it.\n\n&#x200B;\n\nBut at the same time I need to practice Pyspark for the interviews... what I can do?\n\n&#x200B;\n\nthanks", 'author_fullname': 't2_6bblasam', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to leverage my Pyspark skills?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cd15m2', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 17, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 17, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'author_cakeday': True, 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714075480.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I wanna improve my programming skills and to optimize loading large files of data in Pyspark but I feel if you dont work in a company with Databricks, big paid clusters... it&#39;s so hard to practice it.</p>\n\n<p>&#x200B;</p>\n\n<p>But at the same time I need to practice Pyspark for the interviews... what I can do?</p>\n\n<p>&#x200B;</p>\n\n<p>thanks</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cd15m2', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Irachar'), 'discussion_type': None, 'num_comments': 10, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cd15m2/how_to_leverage_my_pyspark_skills/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cd15m2/how_to_leverage_my_pyspark_skills/', 'subreddit_subscribers': 179075, 'created_utc': 1714075480.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.911+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi folks, sysadmin in a tiny enterprise here, today wearing a data engineer hat :)  \nwas pitted with a task of creating a data warehouse on prem, for BI purposes. C-suite wants some performance and financial data from different departments nicely displayed in series of different dashboards.  \nthe source of data coming in will be couple of local sql db instances(3-4) I do not expect a major amount of data, mainly sales figures and some performance metrics. As cost is a major factor the whole stack has to be opensource. Did a bit of googling and came up with stack as follows:  \nApache Airflow for connection to sql db(source data) postgreSQL as DB, dbt for modelling and Redash for dashboards.  \nDoes the above setup makes sense from the requirement point?   \nI will be the sole implementer and maintainer of this platform so ideally for me would be to have a stack build out of ready made programs, rather than going the Python route and developing some components myself(lack the coding skill and time, my proficiency in Python = being able to edit the code that chatGPT spits out so it somewhat does what I need:)   \nappreciate any advice on this, thanks.', 'author_fullname': 't2_nc7je1zy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Opensource solution for a tiny data warehouse.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccs3vs', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 14, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 14, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714052117.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi folks, sysadmin in a tiny enterprise here, today wearing a data engineer hat :)<br/>\nwas pitted with a task of creating a data warehouse on prem, for BI purposes. C-suite wants some performance and financial data from different departments nicely displayed in series of different dashboards.<br/>\nthe source of data coming in will be couple of local sql db instances(3-4) I do not expect a major amount of data, mainly sales figures and some performance metrics. As cost is a major factor the whole stack has to be opensource. Did a bit of googling and came up with stack as follows:<br/>\nApache Airflow for connection to sql db(source data) postgreSQL as DB, dbt for modelling and Redash for dashboards.<br/>\nDoes the above setup makes sense from the requirement point?<br/>\nI will be the sole implementer and maintainer of this platform so ideally for me would be to have a stack build out of ready made programs, rather than going the Python route and developing some components myself(lack the coding skill and time, my proficiency in Python = being able to edit the code that chatGPT spits out so it somewhat does what I need:)<br/>\nappreciate any advice on this, thanks.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ccs3vs', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='LeatherPuzzled3855'), 'discussion_type': None, 'num_comments': 35, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccs3vs/opensource_solution_for_a_tiny_data_warehouse/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccs3vs/opensource_solution_for_a_tiny_data_warehouse/', 'subreddit_subscribers': 179075, 'created_utc': 1714052117.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.911+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I may need to switch back to using Windows. I’ve been on a Mac and/or Linux at work and home for the last decade or so. I *think* the last version of Windows I used was Vista. I’m far more comfortable in the CLI than the GUI.\n\nI understand Windows has WSL now. Where is that at from a capability perspective? Can I simply use WSL and Firefox and ignore all the other ‘Windows stuff’? As in have a couple of full screen spaces, and just flick between the two? For the odd GUI app I use, does WSL support a GUI?\n\nDon’t really want to need to learn a new OS if I don’t need to.', 'author_fullname': 't2_ojr03vx2i', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Using WSL2 full time', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cdc878', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 15, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': '9ecf3c88-e787-11ed-957e-de1616aeae13', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 15, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714105160.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I may need to switch back to using Windows. I’ve been on a Mac and/or Linux at work and home for the last decade or so. I <em>think</em> the last version of Windows I used was Vista. I’m far more comfortable in the CLI than the GUI.</p>\n\n<p>I understand Windows has WSL now. Where is that at from a capability perspective? Can I simply use WSL and Firefox and ignore all the other ‘Windows stuff’? As in have a couple of full screen spaces, and just flick between the two? For the odd GUI app I use, does WSL support a GUI?</p>\n\n<p>Don’t really want to need to learn a new OS if I don’t need to.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Data Engineering Manager', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cdc878', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='nydasco'), 'discussion_type': None, 'num_comments': 11, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1cdc878/using_wsl2_full_time/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cdc878/using_wsl2_full_time/', 'subreddit_subscribers': 179075, 'created_utc': 1714105160.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.912+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'referring to the "choose boring technology" essay.  would you consider spark as such a technology.  im talking about  building on prem spark cluster and * not*  using a managed service.  \nfor a company that knows its way around high performance cpp and python. (no java).\n\nis getting into  the spark advanture "boring"?\nor maybe just dask jobs are good enough?\n\n ', 'author_fullname': 't2_3203wt9n', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'is spark a "boring " technology?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cdg89d', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.78, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714120329.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>referring to the &quot;choose boring technology&quot; essay.  would you consider spark as such a technology.  im talking about  building on prem spark cluster and * not*  using a managed service.<br/>\nfor a company that knows its way around high performance cpp and python. (no java).</p>\n\n<p>is getting into  the spark advanture &quot;boring&quot;?\nor maybe just dask jobs are good enough?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cdg89d', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Automatic-Law2404'), 'discussion_type': None, 'num_comments': 15, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cdg89d/is_spark_a_boring_technology/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cdg89d/is_spark_a_boring_technology/', 'subreddit_subscribers': 179075, 'created_utc': 1714120329.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.912+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi Data Engineers,\n\nI hope to get some insight here as I am making a career transition decision. I have formerly been a systems administrator but I quit because of the on-call requirements. I am currently enrolled in University to get my Data Science degree and I just recently got interested about a bootcamp to become a Data Engineer. My question is do you have to be on-call as a Data Engineer? Asking the bootcamp they told me that most Data Engineer jobs do not require you to be on-call and that you work only during normal business hours.\n\nThanks in advance for your answers!', 'author_fullname': 't2_61eph5xa', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Engineering Requirements vs. Data Science - Career Decision', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cd6v3c', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.77, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714089498.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi Data Engineers,</p>\n\n<p>I hope to get some insight here as I am making a career transition decision. I have formerly been a systems administrator but I quit because of the on-call requirements. I am currently enrolled in University to get my Data Science degree and I just recently got interested about a bootcamp to become a Data Engineer. My question is do you have to be on-call as a Data Engineer? Asking the bootcamp they told me that most Data Engineer jobs do not require you to be on-call and that you work only during normal business hours.</p>\n\n<p>Thanks in advance for your answers!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1cd6v3c', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='motiontrading'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cd6v3c/data_engineering_requirements_vs_data_science/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cd6v3c/data_engineering_requirements_vs_data_science/', 'subreddit_subscribers': 179075, 'created_utc': 1714089498.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.913+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I feel like I’m overthinking this, but what’s the general opinion regarding Mage? In a recent post, someone asked about the current state of orchestration and got several good replies. I lead a small data team who just went through our evaluation process so I replied with my experience running Mage in production. However, my reply received far more downvotes than I was expecting. We’re 6 months into our deployment and it’s gone well so far, but the swift negative reaction has me second-guessing myself. We’re not so far along that we can’t pivot but it would be inconvenient. Did I miss something?', 'author_fullname': 't2_95pplng2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Sanity Check: Mage', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cda190', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714098331.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I feel like I’m overthinking this, but what’s the general opinion regarding Mage? In a recent post, someone asked about the current state of orchestration and got several good replies. I lead a small data team who just went through our evaluation process so I replied with my experience running Mage in production. However, my reply received far more downvotes than I was expecting. We’re 6 months into our deployment and it’s gone well so far, but the swift negative reaction has me second-guessing myself. We’re not so far along that we can’t pivot but it would be inconvenient. Did I miss something?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cda190', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Lord_Lloydd'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cda190/sanity_check_mage/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cda190/sanity_check_mage/', 'subreddit_subscribers': 179075, 'created_utc': 1714098331.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.913+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm a Data Analyst/Engineer at a relatively small (500 employee) healthcare organization. At first a large portion of my duties were to write custom SQL queries to extract SSMS data from a 3rd party org that houses our data. This either goes into operational/grant reporting or dashboards in PowerBI.\n\nFrankly, I literally cannot make another dashboard at this point. Call it burn out call it what you will but I find my eyes glazing over whenever someone talks about a shiny custom dashboard that they won't bother to learn to use and will just forget about 20 minutes after I present it. Strangely enough though I'm not sick of SQL and unfortunately what little Python I know is getting rusty.\n\nI approached our CTO about creating a cloud data infrastructure in Azure. We're an NPO so we get $3000 for the platform annually. He pretty much gave me the keys to the kingdom to work up some POC but frankly I don't know where the hell to start.\n\nSo far all I've done is automate a few pipelines to pull/transform data from SSMS with ADF and then use Logic Apps to drop csv in an external SFTP server. I've started curating data marts by creating Azure SQL Db tables to extract and house only our data from the 3rd party org in semi-functional Datamarts but these seem lateral at best.\n\nThere are several external orgs that house data that we don't have access to in our Epic  system/3rd party SSMS and I think extracting this and enriching our data would be a great starting point. I'm trying to build something useful and scalable (while also being a portfolio builder) and really trying to justify using Databricks in some capacity.\n\nIs there anyone that has had this experience or would be willing to give some advice on how to kick this off/where to start. I took a course for the DP-203 exam but all they really did was fellate Synapse which I'm not even about to start using.\n\n  \n\\*I should rephrase this and say a large majority of our data goes into an EHR (Epic) and realistically there are only like 30-40 providers + support staff inputting any data in Epic. The vast majority of of staff are in operations in other capacities that don't have much to do with any data accessibility. Our data team is 3 Analysts.", 'author_fullname': 't2_a6eqqx3e', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Implementing a cloud data architecture (Azure) at a small/medium healthcare NPO.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccy0s6', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1714083960.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714068453.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m a Data Analyst/Engineer at a relatively small (500 employee) healthcare organization. At first a large portion of my duties were to write custom SQL queries to extract SSMS data from a 3rd party org that houses our data. This either goes into operational/grant reporting or dashboards in PowerBI.</p>\n\n<p>Frankly, I literally cannot make another dashboard at this point. Call it burn out call it what you will but I find my eyes glazing over whenever someone talks about a shiny custom dashboard that they won&#39;t bother to learn to use and will just forget about 20 minutes after I present it. Strangely enough though I&#39;m not sick of SQL and unfortunately what little Python I know is getting rusty.</p>\n\n<p>I approached our CTO about creating a cloud data infrastructure in Azure. We&#39;re an NPO so we get $3000 for the platform annually. He pretty much gave me the keys to the kingdom to work up some POC but frankly I don&#39;t know where the hell to start.</p>\n\n<p>So far all I&#39;ve done is automate a few pipelines to pull/transform data from SSMS with ADF and then use Logic Apps to drop csv in an external SFTP server. I&#39;ve started curating data marts by creating Azure SQL Db tables to extract and house only our data from the 3rd party org in semi-functional Datamarts but these seem lateral at best.</p>\n\n<p>There are several external orgs that house data that we don&#39;t have access to in our Epic  system/3rd party SSMS and I think extracting this and enriching our data would be a great starting point. I&#39;m trying to build something useful and scalable (while also being a portfolio builder) and really trying to justify using Databricks in some capacity.</p>\n\n<p>Is there anyone that has had this experience or would be willing to give some advice on how to kick this off/where to start. I took a course for the DP-203 exam but all they really did was fellate Synapse which I&#39;m not even about to start using.</p>\n\n<p>*I should rephrase this and say a large majority of our data goes into an EHR (Epic) and realistically there are only like 30-40 providers + support staff inputting any data in Epic. The vast majority of of staff are in operations in other capacities that don&#39;t have much to do with any data accessibility. Our data team is 3 Analysts.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1ccy0s6', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Special-Salamander10'), 'discussion_type': None, 'num_comments': 13, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccy0s6/implementing_a_cloud_data_architecture_azure_at_a/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccy0s6/implementing_a_cloud_data_architecture_azure_at_a/', 'subreddit_subscribers': 179075, 'created_utc': 1714068453.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.914+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi everyone, I currently want to do a data pipeline for an app that users can insert any news url(tech blog site, news site,...) that they want to get notification if that site upload a new blog. What Im going to do is:\n\n\\- use scrapy to scrape all url that appear on the input url (my scale is going to be 1000s of url a day). Output of 1 scraped url is all the urls that appear on that site(example. Link to home, link to blogs, links to pages,...)   \n\\- after scrape through 1000s of url I will store the output in S3 bucket (output of scraped url), I will process that to get what is the blogs url in the output and I will save it into database Ex. Reddit. 'com has 20 blog links  \n \\- i will run that loop each day to refresh if there are any site that post new blogs and update my database\n\n\\- in addition, i have to serve data for my DS to build a model to recommend users another blogs base on there urls subscription they want to get notified  \nWhat will you do if you approach this problem, is my plan for this problem optimized ?  \nWhat should I do to serve the data for the DS smoothly?  \nAppreciate every advice !", 'author_fullname': 't2_hk3r67cz', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data pipeline for a new blogs/ articles notification of Tech blogs', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cd89wr', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714093328.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone, I currently want to do a data pipeline for an app that users can insert any news url(tech blog site, news site,...) that they want to get notification if that site upload a new blog. What Im going to do is:</p>\n\n<p>- use scrapy to scrape all url that appear on the input url (my scale is going to be 1000s of url a day). Output of 1 scraped url is all the urls that appear on that site(example. Link to home, link to blogs, links to pages,...)<br/>\n- after scrape through 1000s of url I will store the output in S3 bucket (output of scraped url), I will process that to get what is the blogs url in the output and I will save it into database Ex. Reddit. &#39;com has 20 blog links<br/>\n - i will run that loop each day to refresh if there are any site that post new blogs and update my database</p>\n\n<p>- in addition, i have to serve data for my DS to build a model to recommend users another blogs base on there urls subscription they want to get notified<br/>\nWhat will you do if you approach this problem, is my plan for this problem optimized ?<br/>\nWhat should I do to serve the data for the DS smoothly?<br/>\nAppreciate every advice !</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cd89wr', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='FriendshipEastern291'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cd89wr/data_pipeline_for_a_new_blogs_articles/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cd89wr/data_pipeline_for_a_new_blogs_articles/', 'subreddit_subscribers': 179075, 'created_utc': 1714093328.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.914+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi everyone,\n\nA system migration project I’m working on right now has an issue where changes are being made to field names and shifting fields from one table to another as they continue to design the system while we are building curated datasets/implementing at the same time. \n\nThe problem is, they’ll change a field name from say, field_name_10 to field_name_20, or shift a field from table_4 to table_8, without letting our team or other downstream teams know. As a result, our scripts will fail and we have to find where the change was made to. There are multiple changes being made during the week/month, and we have over 50 tables in our data warehouse so we cannot check every single table for changes. \n\nMy question is, how have some of you dealt with this? How are changes communicated to you? Any particular tools or apps that we can use to help with this? I’m looking for something that can maybe notify us of changes made if it directly affect us/our tables in the DW. \n\nI thought of using the github versioning stuff, but it doesn’t seem like the best option since we aren’t all using the same schemas. \n\n\n', 'author_fullname': 't2_tna1k085', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How do you handle changes upstream to field names that affect your scripts downstream?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cd4p69', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714084090.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone,</p>\n\n<p>A system migration project I’m working on right now has an issue where changes are being made to field names and shifting fields from one table to another as they continue to design the system while we are building curated datasets/implementing at the same time. </p>\n\n<p>The problem is, they’ll change a field name from say, field_name_10 to field_name_20, or shift a field from table_4 to table_8, without letting our team or other downstream teams know. As a result, our scripts will fail and we have to find where the change was made to. There are multiple changes being made during the week/month, and we have over 50 tables in our data warehouse so we cannot check every single table for changes. </p>\n\n<p>My question is, how have some of you dealt with this? How are changes communicated to you? Any particular tools or apps that we can use to help with this? I’m looking for something that can maybe notify us of changes made if it directly affect us/our tables in the DW. </p>\n\n<p>I thought of using the github versioning stuff, but it doesn’t seem like the best option since we aren’t all using the same schemas. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cd4p69', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='miserablywinning'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cd4p69/how_do_you_handle_changes_upstream_to_field_names/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cd4p69/how_do_you_handle_changes_upstream_to_field_names/', 'subreddit_subscribers': 179075, 'created_utc': 1714084090.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.915+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi everyone, that's say we have some operational database (mysql) and to do some ELT to directly dump the data into a OLAP.  I am not sure what's the proper tool to use here. I have played around with some potential tools here,\n\n* Airbyte (no code solution, it works for how's the scalability?)\n* Flink and flink cdc\n* Meltano (yaml-based but not a lot of people use it)\n* any other recommendation?\n\nJust trying to see the pro and cons here. Thanks", 'author_fullname': 't2_12wrnq', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Sync tables from Mysql to any OLAP', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cd3xyl', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714081948.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone, that&#39;s say we have some operational database (mysql) and to do some ELT to directly dump the data into a OLAP.  I am not sure what&#39;s the proper tool to use here. I have played around with some potential tools here,</p>\n\n<ul>\n<li>Airbyte (no code solution, it works for how&#39;s the scalability?)</li>\n<li>Flink and flink cdc</li>\n<li>Meltano (yaml-based but not a lot of people use it)</li>\n<li>any other recommendation?</li>\n</ul>\n\n<p>Just trying to see the pro and cons here. Thanks</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cd3xyl', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='diceHots'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cd3xyl/sync_tables_from_mysql_to_any_olap/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cd3xyl/sync_tables_from_mysql_to_any_olap/', 'subreddit_subscribers': 179075, 'created_utc': 1714081948.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.915+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi all,\n\nI've been exploring various NoSQL databases that effectively handle large datasets (1 entry per second) and provide robust support for location data and big data sets. Recently, I've been delving into Neo4j, and I got inspired by its graph-based model, which seems to offer significant advantages for complex queries that involve relationships and spatial data.\n\nMy thought is in between neo4j and elastic search. Is the approach of Neo4J a logical one? What are your experiences?\n\nKind regards,\n\n\\_\\_bdude", 'author_fullname': 't2_xc6gwciem', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Neo4j as a NoSQL database for large data and location data', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cd1ce6', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714075899.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all,</p>\n\n<p>I&#39;ve been exploring various NoSQL databases that effectively handle large datasets (1 entry per second) and provide robust support for location data and big data sets. Recently, I&#39;ve been delving into Neo4j, and I got inspired by its graph-based model, which seems to offer significant advantages for complex queries that involve relationships and spatial data.</p>\n\n<p>My thought is in between neo4j and elastic search. Is the approach of Neo4J a logical one? What are your experiences?</p>\n\n<p>Kind regards,</p>\n\n<p>__bdude</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cd1ce6', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='__bdude'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cd1ce6/neo4j_as_a_nosql_database_for_large_data_and/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cd1ce6/neo4j_as_a_nosql_database_for_large_data_and/', 'subreddit_subscribers': 179075, 'created_utc': 1714075899.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.916+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'If you see peer reviews as a bit of a check box process that you just need to do, please take 5 minutes to read my article on their value. Hopefully I can change your mind.\n\nNote: I learned my lesson with my last link to Medium, so this is direct to the friends and family access. Should be no log in required.', 'author_fullname': 't2_ojr03vx2i', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Peer Reviews - they actually add more value than you think', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 93, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cdgbjw', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': 'transparent', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': '9ecf3c88-e787-11ed-957e-de1616aeae13', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/dmkonncT356rb5VlNBWKZDdCOYLLYEAjxdrkkanKurw.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1714120719.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'medium.com', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>If you see peer reviews as a bit of a check box process that you just need to do, please take 5 minutes to read my article on their value. Hopefully I can change your mind.</p>\n\n<p>Note: I learned my lesson with my last link to Medium, so this is direct to the friends and family access. Should be no log in required.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://medium.com/@nydas/the-underrated-value-of-peer-reviews-in-data-delivery-79092e8040b2?source=friends_link&sk=d53ace6f7b4a962d5491ae4623f3fea8', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/4xSFKBWlka_zo40KZsTowejvQvFWDSF9D79gVmlNbeI.jpg?auto=webp&s=2af4fb0f73ed1448bc83a6d0f56959913356fb8b', 'width': 1000, 'height': 667}, 'resolutions': [{'url': 'https://external-preview.redd.it/4xSFKBWlka_zo40KZsTowejvQvFWDSF9D79gVmlNbeI.jpg?width=108&crop=smart&auto=webp&s=a024a8f2d7de32fd2f812e9f06624e7e71b1e6b9', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/4xSFKBWlka_zo40KZsTowejvQvFWDSF9D79gVmlNbeI.jpg?width=216&crop=smart&auto=webp&s=f53562c297a5b691ad9836dc10487741e57f0f36', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/4xSFKBWlka_zo40KZsTowejvQvFWDSF9D79gVmlNbeI.jpg?width=320&crop=smart&auto=webp&s=5434e9156f5888eff9a131335d26da1e83cb3246', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/4xSFKBWlka_zo40KZsTowejvQvFWDSF9D79gVmlNbeI.jpg?width=640&crop=smart&auto=webp&s=654784e28e7870a56c02bbb7ea53613cc853d808', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/4xSFKBWlka_zo40KZsTowejvQvFWDSF9D79gVmlNbeI.jpg?width=960&crop=smart&auto=webp&s=04acf03cf986d17481f9895926835a23ba3e0d98', 'width': 960, 'height': 640}], 'variants': {}, 'id': 'Wdj2TNyvSshlNAwihdBNfw1lgDBdyGOI4v9BE6zme-I'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Data Engineering Manager', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1cdgbjw', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='nydasco'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1cdgbjw/peer_reviews_they_actually_add_more_value_than/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://medium.com/@nydas/the-underrated-value-of-peer-reviews-in-data-delivery-79092e8040b2?source=friends_link&sk=d53ace6f7b4a962d5491ae4623f3fea8', 'subreddit_subscribers': 179075, 'created_utc': 1714120719.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.916+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': ' Just delivered a new Data Platform with massive success.\n 36 months of hard work, within a global team spreading across multiple timezone.\n \n After delivery, we got huge positive feedback but we also got overwhelmed with question around provenance, glossary info, metadata desc, process flow, freshess, etc \n  Were data consumers want to plug in there process into our data streams and consume them, all in all fair questions.\n\n So that is the problem.\n\n Solution Data Catalogue solution.\n\nMe as a definitive geek, went and implement one as POC.  In this case  I used datahub. \n\nIssue i have. \nGetting pushback from leadership on this, and i am super confused about it.\nThey prefer doing lineage in drawio in confluence.\nTransformation mapping in excell in some obscure sharepoint folder.\nAnd the governance manager does not even know tools like datahub, open metadata, Amundsen even exists... wtf.\n\nI love where i work but this kinda raised an alarm. \n\nAnyway,  i still wanna add Datahub to our stack and prove is worth it. \n\nHow woukd you tackle this?', 'author_fullname': 't2_nxu067bi', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Catalogue Buy In', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cd8xbl', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714095187.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Just delivered a new Data Platform with massive success.\n 36 months of hard work, within a global team spreading across multiple timezone.</p>\n\n<p>After delivery, we got huge positive feedback but we also got overwhelmed with question around provenance, glossary info, metadata desc, process flow, freshess, etc \n  Were data consumers want to plug in there process into our data streams and consume them, all in all fair questions.</p>\n\n<p>So that is the problem.</p>\n\n<p>Solution Data Catalogue solution.</p>\n\n<p>Me as a definitive geek, went and implement one as POC.  In this case  I used datahub. </p>\n\n<p>Issue i have. \nGetting pushback from leadership on this, and i am super confused about it.\nThey prefer doing lineage in drawio in confluence.\nTransformation mapping in excell in some obscure sharepoint folder.\nAnd the governance manager does not even know tools like datahub, open metadata, Amundsen even exists... wtf.</p>\n\n<p>I love where i work but this kinda raised an alarm. </p>\n\n<p>Anyway,  i still wanna add Datahub to our stack and prove is worth it. </p>\n\n<p>How woukd you tackle this?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cd8xbl', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='InsightByte'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cd8xbl/data_catalogue_buy_in/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cd8xbl/data_catalogue_buy_in/', 'subreddit_subscribers': 179075, 'created_utc': 1714095187.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.917+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': ' For my company, I want to build a lakehouse with Databricks in Azure. However, I am not sure how to name all my unity catalogs and organize the dev/prod workspaces. \n\n&#x200B;\n\nMy question to you is: **Am I doing this right?** I\'m looking for best practices in organizing my workspace/Databricks structure without having to re-do it in half a year because I missed something important. I\'m half-sure why I\'m doing it like this and I want to start off correctly.\n\n# Background\n\n* We have two seperate businesses with their own data: Digital and Sales.\n* Currently I have a resourcegroup named rg-lakehouse-dev.\n   * I have a dev storage account in there and a dev Databricks workspace.\n\n**Current setup ADLS:** 4 containers:\n\n* 01-landing (raw data, any format)\n* 02-bronze (raw data to delta)\n* 03-silver (delta tables with better column types etc)\n* 04-gold (end data)\n* Each container has a folder named "Digital" and "Sales" with external locations linked to each folder in Databricks for Unity Catalog (so 8 external locations - digital & sales, dev & prod, landing + bronze + silver + gold, so **adls-digital-landing** for example, or **adls-sales-silver**)\n\n&#x200B;\n\nhttps://preview.redd.it/tszh22r1wnwc1.png?width=176&format=png&auto=webp&s=482d4a82ce1245e05e4833f46f564d7f9b914d7c\n\n**Current setup DataBricks, dev:** 6 catalogs:\n\n* digital\\_dev\\_bronze\n* digital\\_dev\\_silver\n* digital\\_dev\\_gold\n* sales\\_dev\\_bronze\n* sales\\_dev\\_silver\n* sales\\_dev\\_gold\n\n&#x200B;\n\nhttps://preview.redd.it/sri17z13wnwc1.png?width=189&format=png&auto=webp&s=4c57dc5fee7bf909cde04b6247ff8d929c6920a7\n\nThen I want to fill these catalogs with data for dev. The catalogs for production will not be linked to this workspace account.\n\n&#x200B;\n\nIs this a best-practice approach? Would you advice me to do it differently?\n\n&#x200B;\n\nThanks! Looking forward to your ideas.', 'author_fullname': 't2_28svzb2d', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Databricks - Lakehouse setup - How to organize Unity Catalog + Medallions and ADLS?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'tszh22r1wnwc1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 177, 'x': 108, 'u': 'https://preview.redd.it/tszh22r1wnwc1.png?width=108&crop=smart&auto=webp&s=30ca9541109001943e007f36b1a60282556491ce'}], 's': {'y': 289, 'x': 176, 'u': 'https://preview.redd.it/tszh22r1wnwc1.png?width=176&format=png&auto=webp&s=482d4a82ce1245e05e4833f46f564d7f9b914d7c'}, 'id': 'tszh22r1wnwc1'}, 'sri17z13wnwc1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 175, 'x': 108, 'u': 'https://preview.redd.it/sri17z13wnwc1.png?width=108&crop=smart&auto=webp&s=6e3f7c9f9f3e63c19100d6d4359d0817c31e6d1b'}], 's': {'y': 307, 'x': 189, 'u': 'https://preview.redd.it/sri17z13wnwc1.png?width=189&format=png&auto=webp&s=4c57dc5fee7bf909cde04b6247ff8d929c6920a7'}, 'id': 'sri17z13wnwc1'}}, 'name': 't3_1ccx5yc', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/A3AkqrprUzLqHWMRLUV9_SAYzziatHaXN6-8dZuH-gA.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714066666.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>For my company, I want to build a lakehouse with Databricks in Azure. However, I am not sure how to name all my unity catalogs and organize the dev/prod workspaces. </p>\n\n<p>&#x200B;</p>\n\n<p>My question to you is: <strong>Am I doing this right?</strong> I&#39;m looking for best practices in organizing my workspace/Databricks structure without having to re-do it in half a year because I missed something important. I&#39;m half-sure why I&#39;m doing it like this and I want to start off correctly.</p>\n\n<h1>Background</h1>\n\n<ul>\n<li>We have two seperate businesses with their own data: Digital and Sales.</li>\n<li>Currently I have a resourcegroup named rg-lakehouse-dev.\n\n<ul>\n<li>I have a dev storage account in there and a dev Databricks workspace.</li>\n</ul></li>\n</ul>\n\n<p><strong>Current setup ADLS:</strong> 4 containers:</p>\n\n<ul>\n<li>01-landing (raw data, any format)</li>\n<li>02-bronze (raw data to delta)</li>\n<li>03-silver (delta tables with better column types etc)</li>\n<li>04-gold (end data)</li>\n<li>Each container has a folder named &quot;Digital&quot; and &quot;Sales&quot; with external locations linked to each folder in Databricks for Unity Catalog (so 8 external locations - digital &amp; sales, dev &amp; prod, landing + bronze + silver + gold, so <strong>adls-digital-landing</strong> for example, or <strong>adls-sales-silver</strong>)</li>\n</ul>\n\n<p>&#x200B;</p>\n\n<p><a href="https://preview.redd.it/tszh22r1wnwc1.png?width=176&amp;format=png&amp;auto=webp&amp;s=482d4a82ce1245e05e4833f46f564d7f9b914d7c">https://preview.redd.it/tszh22r1wnwc1.png?width=176&amp;format=png&amp;auto=webp&amp;s=482d4a82ce1245e05e4833f46f564d7f9b914d7c</a></p>\n\n<p><strong>Current setup DataBricks, dev:</strong> 6 catalogs:</p>\n\n<ul>\n<li>digital_dev_bronze</li>\n<li>digital_dev_silver</li>\n<li>digital_dev_gold</li>\n<li>sales_dev_bronze</li>\n<li>sales_dev_silver</li>\n<li>sales_dev_gold</li>\n</ul>\n\n<p>&#x200B;</p>\n\n<p><a href="https://preview.redd.it/sri17z13wnwc1.png?width=189&amp;format=png&amp;auto=webp&amp;s=4c57dc5fee7bf909cde04b6247ff8d929c6920a7">https://preview.redd.it/sri17z13wnwc1.png?width=189&amp;format=png&amp;auto=webp&amp;s=4c57dc5fee7bf909cde04b6247ff8d929c6920a7</a></p>\n\n<p>Then I want to fill these catalogs with data for dev. The catalogs for production will not be linked to this workspace account.</p>\n\n<p>&#x200B;</p>\n\n<p>Is this a best-practice approach? Would you advice me to do it differently?</p>\n\n<p>&#x200B;</p>\n\n<p>Thanks! Looking forward to your ideas.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ccx5yc', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Allstarbowser'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccx5yc/databricks_lakehouse_setup_how_to_organize_unity/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccx5yc/databricks_lakehouse_setup_how_to_organize_unity/', 'subreddit_subscribers': 179075, 'created_utc': 1714066666.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.917+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi all,\n\nI’m a data engineer and I’ve been learning the ropes in cloud since starting one and a bit years ago. I moved to a company that advertised predominantly cloud tech with a tiny bit of on-prem, but so far for the third sprint, we’re consumed by on-prem work that I’m not trained in and I’m starting to get fed up.\n\nI’m doing some certifications outside of work for cloud but at the same time considering changing companies as my career trajectory is cloud and not on-prem. Has anyone been in this position? Is there any point in staying too long?\n\nThanks ', 'author_fullname': 't2_bs6bpgld', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Job expectation vs reality', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccrag8', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714049923.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all,</p>\n\n<p>I’m a data engineer and I’ve been learning the ropes in cloud since starting one and a bit years ago. I moved to a company that advertised predominantly cloud tech with a tiny bit of on-prem, but so far for the third sprint, we’re consumed by on-prem work that I’m not trained in and I’m starting to get fed up.</p>\n\n<p>I’m doing some certifications outside of work for cloud but at the same time considering changing companies as my career trajectory is cloud and not on-prem. Has anyone been in this position? Is there any point in staying too long?</p>\n\n<p>Thanks </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1ccrag8', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='JackalTheFulgid'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccrag8/job_expectation_vs_reality/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccrag8/job_expectation_vs_reality/', 'subreddit_subscribers': 179075, 'created_utc': 1714049923.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.918+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'hi I was recently looking at some relatively large dbt models (\\~300 lines) and it had many CTEs which built up intermediate states. I had such a tough time refactoring the query and was wondering "if there was only a way to set a breakpoint here and look at the state of the intermediate table/cte". So im here to ask the community, is there such a tool? (using redshift and databricks for context)\n\nAre we not severely handicapped compared to more general purpose software engineering where this is taken for granted? ', 'author_fullname': 't2_6guf9', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'ability to set breakpoints in sql to debug intermediate states of tables?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cdha2w', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714124785.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>hi I was recently looking at some relatively large dbt models (~300 lines) and it had many CTEs which built up intermediate states. I had such a tough time refactoring the query and was wondering &quot;if there was only a way to set a breakpoint here and look at the state of the intermediate table/cte&quot;. So im here to ask the community, is there such a tool? (using redshift and databricks for context)</p>\n\n<p>Are we not severely handicapped compared to more general purpose software engineering where this is taken for granted? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cdha2w', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='cyberjar09'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cdha2w/ability_to_set_breakpoints_in_sql_to_debug/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cdha2w/ability_to_set_breakpoints_in_sql_to_debug/', 'subreddit_subscribers': 179075, 'created_utc': 1714124785.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.918+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm essentially looking for a way to run SQL like queries across our data, that consists of graph data in neo4j (a representation of users, groups they belong to, etc) and MongoDB data that contains richer data about those graph nodes.\n\nWe want to be able to design queries that for example build a list of Users based on conditions that might exist in either the graph or the mongo data store. As a software engineer I'm a bit out of my depth when it comes to data engineering.\n\nOpen question, is there something out there that will let us achieve this? or do I need to start considering building a relational database representation of our data? I'm a bit reluctant to go down the traditional relational SQL avenue because the data structures and types of data we are storing are likely to change quite a bit over time and possibly from customer to customer.", 'author_fullname': 't2_5uri0k1w', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Query engine to combine Graph and NoSQL Data', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cd8sem', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714094790.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m essentially looking for a way to run SQL like queries across our data, that consists of graph data in neo4j (a representation of users, groups they belong to, etc) and MongoDB data that contains richer data about those graph nodes.</p>\n\n<p>We want to be able to design queries that for example build a list of Users based on conditions that might exist in either the graph or the mongo data store. As a software engineer I&#39;m a bit out of my depth when it comes to data engineering.</p>\n\n<p>Open question, is there something out there that will let us achieve this? or do I need to start considering building a relational database representation of our data? I&#39;m a bit reluctant to go down the traditional relational SQL avenue because the data structures and types of data we are storing are likely to change quite a bit over time and possibly from customer to customer.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cd8sem', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Hephaestite'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cd8sem/query_engine_to_combine_graph_and_nosql_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cd8sem/query_engine_to_combine_graph_and_nosql_data/', 'subreddit_subscribers': 179075, 'created_utc': 1714094790.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.919+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I was toying around with the stack overflow data dumps ( in my case Law Exchange since it was smaller ) mainly trying to implement TF-IDF in plpgsql. The data came in these big denormalized XML files, and I while its clear why they would decide to denormalize the relations, I also kinda was wondering if its posible to have the cake and both eat it with a implementation like this: \n\n • Data is stored in a normalized form\n\n • The database has a materialized view with a query that denormalizes this data into itself.\n\n • Any read query is sent to the materialized form, where it will be a lot faster to query all the relevant data without extra joins.  \n\n • Any writes are sent to the Normalized relationships who will perform a easy write that can actually be given efficient Foreign Key constraints, from there a trigger will be fired up after the write and this trigger will modify the materialized view apropriatly ( not rerun the base query but rather insert/update a row )\n\n • Database backups will truncate the materialized view and due to the normalized data structure backups will become smaller, in the case of a need to restore from backup, the materialized view query can be rerun.\n\nFor me this seems like a sitatuion without a downside,\n\n • You get smaller file sizes (since for every prod database Im assuming there are at least a few in backup). \n\n • Intuitive consistency checks that dont need to be implimented in the application layer. \n\n • A much more logical and easy to work with schema.\n\n • Slight performance improvments on writes ( if I'm not wrong, since the extra write to the view could eat that, either way for such a read intensive app the benifit is pretty meaningless )\n\n • Cott is happy\n\nOf course this dosnt change the fact that those guys know what they are doing and probably have a good reason to do it this way, My question is, is such a implementatiom valid? Am I missing something here?\n", 'author_fullname': 't2_st17thy4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Question regarding relation denormalization ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cd2hq4', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714078499.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I was toying around with the stack overflow data dumps ( in my case Law Exchange since it was smaller ) mainly trying to implement TF-IDF in plpgsql. The data came in these big denormalized XML files, and I while its clear why they would decide to denormalize the relations, I also kinda was wondering if its posible to have the cake and both eat it with a implementation like this: </p>\n\n<p>• Data is stored in a normalized form</p>\n\n<p>• The database has a materialized view with a query that denormalizes this data into itself.</p>\n\n<p>• Any read query is sent to the materialized form, where it will be a lot faster to query all the relevant data without extra joins.  </p>\n\n<p>• Any writes are sent to the Normalized relationships who will perform a easy write that can actually be given efficient Foreign Key constraints, from there a trigger will be fired up after the write and this trigger will modify the materialized view apropriatly ( not rerun the base query but rather insert/update a row )</p>\n\n<p>• Database backups will truncate the materialized view and due to the normalized data structure backups will become smaller, in the case of a need to restore from backup, the materialized view query can be rerun.</p>\n\n<p>For me this seems like a sitatuion without a downside,</p>\n\n<p>• You get smaller file sizes (since for every prod database Im assuming there are at least a few in backup). </p>\n\n<p>• Intuitive consistency checks that dont need to be implimented in the application layer. </p>\n\n<p>• A much more logical and easy to work with schema.</p>\n\n<p>• Slight performance improvments on writes ( if I&#39;m not wrong, since the extra write to the view could eat that, either way for such a read intensive app the benifit is pretty meaningless )</p>\n\n<p>• Cott is happy</p>\n\n<p>Of course this dosnt change the fact that those guys know what they are doing and probably have a good reason to do it this way, My question is, is such a implementatiom valid? Am I missing something here?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cd2hq4', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='xyzb206'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cd2hq4/question_regarding_relation_denormalization/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cd2hq4/question_regarding_relation_denormalization/', 'subreddit_subscribers': 179075, 'created_utc': 1714078499.0, 'num_crossposts': 1, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.919+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Not sure that's the best group for the question, but here I go: I've been working in data, ML and AI for years. I know data science, how to build AI/ML-using apps, prepare data, etc. I specialize on Azure.\n\nWhat's the fastest way for me to learn about networking/ sec/ generally infra? Any good books? I feel that that's something that's a blocker for me when I design architectures.", 'author_fullname': 't2_ti6b0o4i', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Any good resources to learn about networking/sec/infra as a data engineer painlessly?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cdgi6u', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714121517.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Not sure that&#39;s the best group for the question, but here I go: I&#39;ve been working in data, ML and AI for years. I know data science, how to build AI/ML-using apps, prepare data, etc. I specialize on Azure.</p>\n\n<p>What&#39;s the fastest way for me to learn about networking/ sec/ generally infra? Any good books? I feel that that&#39;s something that&#39;s a blocker for me when I design architectures.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cdgi6u', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='user2401372'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cdgi6u/any_good_resources_to_learn_about/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cdgi6u/any_good_resources_to_learn_about/', 'subreddit_subscribers': 179075, 'created_utc': 1714121517.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.919+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I wanted to know that if I'm extracting an image from the video can I save it under the same series as of the video or need to have a new Series UID for the images\xa0and if I need to create a new series and extracting another image(with different measurements) from the same video, should that image be saved in the same Series UID as of the newly created UID for the first frame.\n\nAnd if there is 2 videos under the same series UID, do I need to keep the series of extracted images from the both the videos same or they need to differ, EX - I've got a OS and OD scans under the same Series, do extracted images from both the video be under the same series UID or different as for my guess it should be under same series UID and shouldn't create a new one. It would be of great help if someone can help me out on this one as I've been stuck on this for a while now and unable to find any leads on official NEMA documentation.", 'author_fullname': 't2_lgm3nij3g', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Series UID for images extracted from a Multi-frame DICOM image(video)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cdf4f4', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714115666.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I wanted to know that if I&#39;m extracting an image from the video can I save it under the same series as of the video or need to have a new Series UID for the images\xa0and if I need to create a new series and extracting another image(with different measurements) from the same video, should that image be saved in the same Series UID as of the newly created UID for the first frame.</p>\n\n<p>And if there is 2 videos under the same series UID, do I need to keep the series of extracted images from the both the videos same or they need to differ, EX - I&#39;ve got a OS and OD scans under the same Series, do extracted images from both the video be under the same series UID or different as for my guess it should be under same series UID and shouldn&#39;t create a new one. It would be of great help if someone can help me out on this one as I&#39;ve been stuck on this for a while now and unable to find any leads on official NEMA documentation.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cdf4f4', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Past_Key9901'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cdf4f4/series_uid_for_images_extracted_from_a_multiframe/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cdf4f4/series_uid_for_images_extracted_from_a_multiframe/', 'subreddit_subscribers': 179075, 'created_utc': 1714115666.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.920+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I need some help in coding/building the following scenario since I'm new to this profession,\n\nI have a DAG which should be triggered when a file is dropped in a GCS bucket folder. This can be achieved by an event driven cloud Function. Is it possible to suppress the firing of the cloud function until the DAG run is complete and make it active once the DAG run is complete. A step by step explanation of the solution will be greatly appreciated.", 'author_fullname': 't2_nh7i4oh0r', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Cloud Function to trigger Cloud composer DAG', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cd4ouc', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714084068.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I need some help in coding/building the following scenario since I&#39;m new to this profession,</p>\n\n<p>I have a DAG which should be triggered when a file is dropped in a GCS bucket folder. This can be achieved by an event driven cloud Function. Is it possible to suppress the firing of the cloud function until the DAG run is complete and make it active once the DAG run is complete. A step by step explanation of the solution will be greatly appreciated.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cd4ouc', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='brittlet'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cd4ouc/cloud_function_to_trigger_cloud_composer_dag/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cd4ouc/cloud_function_to_trigger_cloud_composer_dag/', 'subreddit_subscribers': 179075, 'created_utc': 1714084068.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.920+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Not sure I\'m in the right place, I checked out the database sub but it doesn\'t seem to be as active.  I started a job a few months (accounting role), in previous places of employment had few problems accessing data via power query for use in excel and for use in power bi.  As background, I know enough basic coding and sql to be dangerous and google is my friend.  Current company uses Sage  with ODBC.  They have the US Sage application and the UK Sage application stored on an Azure cloud server.  Access is via dns router (i think).  To query data and refresh reports I have to use a terminal server and the dns connection is set up as a "silent" (??) Connection to the ODBC database, anyone that wants to refresh reports also has to have this access point.  They don\'t want more than one person doing this as it puts excessive load on the server.  Thus, we are meeting next week to discuss transition to Sage with SQL database, but from what I have heard this will not solve the problems due to the way it is set up.  My question is what other alternatives are there to easily access data without use of terminal server and having the ability for access by multiple people?  Is there a solution that could potentially employ a gateway connection, or a repository that can be auto updated?  I have never had so many problems with accessing data for use in reports.  FYI, our IT group seems to have very little experience in setting this up, so far their solution has been there isnt one and we just can\'t do it unless we get Sage with SQL.   Any help or solutions I can research would be a lifesaver!!!!', 'author_fullname': 't2_2negkm4w', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Database Access Solution', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cd3saf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714081561.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Not sure I&#39;m in the right place, I checked out the database sub but it doesn&#39;t seem to be as active.  I started a job a few months (accounting role), in previous places of employment had few problems accessing data via power query for use in excel and for use in power bi.  As background, I know enough basic coding and sql to be dangerous and google is my friend.  Current company uses Sage  with ODBC.  They have the US Sage application and the UK Sage application stored on an Azure cloud server.  Access is via dns router (i think).  To query data and refresh reports I have to use a terminal server and the dns connection is set up as a &quot;silent&quot; (??) Connection to the ODBC database, anyone that wants to refresh reports also has to have this access point.  They don&#39;t want more than one person doing this as it puts excessive load on the server.  Thus, we are meeting next week to discuss transition to Sage with SQL database, but from what I have heard this will not solve the problems due to the way it is set up.  My question is what other alternatives are there to easily access data without use of terminal server and having the ability for access by multiple people?  Is there a solution that could potentially employ a gateway connection, or a repository that can be auto updated?  I have never had so many problems with accessing data for use in reports.  FYI, our IT group seems to have very little experience in setting this up, so far their solution has been there isnt one and we just can&#39;t do it unless we get Sage with SQL.   Any help or solutions I can research would be a lifesaver!!!!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cd3saf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='sdsmith0610'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cd3saf/database_access_solution/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cd3saf/database_access_solution/', 'subreddit_subscribers': 179075, 'created_utc': 1714081561.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.921+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I [loved this post](https://www.reddit.com/r/dataengineering/comments/1b04b8j/marry_f_kill_databricks_snowflake_ms_fabric/), but think Fabric didn't have chance against the other two. However, Redshift is more widely adopted and some people love BQ, so I thought I would make the fight more fair", 'author_fullname': 't2_vx67of8l', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'F, Marry, Kill, Befriend; Databricks, Snowflake, Redshift, BigQuery (redo)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1cdjiby', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714132682.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I <a href="https://www.reddit.com/r/dataengineering/comments/1b04b8j/marry_f_kill_databricks_snowflake_ms_fabric/">loved this post</a>, but think Fabric didn&#39;t have chance against the other two. However, Redshift is more widely adopted and some people love BQ, so I thought I would make the fight more fair</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cdjiby', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Hot_Map_7868'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cdjiby/f_marry_kill_befriend_databricks_snowflake/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cdjiby/f_marry_kill_befriend_databricks_snowflake/', 'subreddit_subscribers': 179075, 'created_utc': 1714132682.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.921+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "The problem:  \nWe receive nightly backups of our data from a third party solution we're using. The backups are mongodumps and we get the zipped up version. It needs to be downloaded and loaded to some form of data warehouse.\n\nThe current solution:  \nUsing data bricks to download, unzip, store in ADLS and then convert bson/json to CSV so data factory can copy the CSVs to SQL server\n\nThis works, but it's expensive and slow, and I want a resilient solution that can eventually integrate with other systems to ingest more data.\n\nI'm thinking of using Cosmos DB as a stop-gap so the conversion to CSV and copy to SQL server can be bypassed. As yet I haven't been able to get data bricks to upload the BSON files stored in ADLS to cosmos.   \nI've tried using both the Spark version and the Python version but they only seem to copy direct from the source MongoDB, and a 'mongorestore' as I can do on desktop doesn't seem possible because Mongo Tools can't be installed on the data bricks cluster.  \nI've tried setting up a data migration service (that I'm hoping can be triggered daily further down the pipeline) but there seems to be issues with RUs and configs that I can't get past.\n\nIs using Cosmos DB as a stop-gap a plausible solution? And if so, what else can I try to get this working?   ", 'author_fullname': 't2_x609t', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'I need to optimise my workflow extracting mongo data and uploading it to SQL', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cd0r1h', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714074578.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>The problem:<br/>\nWe receive nightly backups of our data from a third party solution we&#39;re using. The backups are mongodumps and we get the zipped up version. It needs to be downloaded and loaded to some form of data warehouse.</p>\n\n<p>The current solution:<br/>\nUsing data bricks to download, unzip, store in ADLS and then convert bson/json to CSV so data factory can copy the CSVs to SQL server</p>\n\n<p>This works, but it&#39;s expensive and slow, and I want a resilient solution that can eventually integrate with other systems to ingest more data.</p>\n\n<p>I&#39;m thinking of using Cosmos DB as a stop-gap so the conversion to CSV and copy to SQL server can be bypassed. As yet I haven&#39;t been able to get data bricks to upload the BSON files stored in ADLS to cosmos.<br/>\nI&#39;ve tried using both the Spark version and the Python version but they only seem to copy direct from the source MongoDB, and a &#39;mongorestore&#39; as I can do on desktop doesn&#39;t seem possible because Mongo Tools can&#39;t be installed on the data bricks cluster.<br/>\nI&#39;ve tried setting up a data migration service (that I&#39;m hoping can be triggered daily further down the pipeline) but there seems to be issues with RUs and configs that I can&#39;t get past.</p>\n\n<p>Is using Cosmos DB as a stop-gap a plausible solution? And if so, what else can I try to get this working?   </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cd0r1h', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Drogen24'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cd0r1h/i_need_to_optimise_my_workflow_extracting_mongo/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cd0r1h/i_need_to_optimise_my_workflow_extracting_mongo/', 'subreddit_subscribers': 179075, 'created_utc': 1714074578.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.922+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Sorry if this type of post isn't allowed, mod's can delete if that is the case.  \nI have a problem at my job where I need to manually read microsoft word reports created by consultants and extract their info into a CSV file. I wanted to do this via python and the docx package but this proved difficult as the reports ended up differing too much from report to report to extract them using the same code.   \nI was wondering if it was possible to use a Generative AI tool to automatically extract the information I need into a CSV from instructions. \n\nAre there any recommendations for new tools that can do this job? I would prefer tools that are free/open source but I can pay if there aren't any free options. I know of some tools like Google's Document AI or Amazon Textract but I was wondering if there were others that would work easier/cheaper", 'author_fullname': 't2_xembpfk16', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Extracting information from a Word Document to CSV', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cczo1s', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714072077.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Sorry if this type of post isn&#39;t allowed, mod&#39;s can delete if that is the case.<br/>\nI have a problem at my job where I need to manually read microsoft word reports created by consultants and extract their info into a CSV file. I wanted to do this via python and the docx package but this proved difficult as the reports ended up differing too much from report to report to extract them using the same code.<br/>\nI was wondering if it was possible to use a Generative AI tool to automatically extract the information I need into a CSV from instructions. </p>\n\n<p>Are there any recommendations for new tools that can do this job? I would prefer tools that are free/open source but I can pay if there aren&#39;t any free options. I know of some tools like Google&#39;s Document AI or Amazon Textract but I was wondering if there were others that would work easier/cheaper</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cczo1s', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='willereid'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cczo1s/extracting_information_from_a_word_document_to_csv/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cczo1s/extracting_information_from_a_word_document_to_csv/', 'subreddit_subscribers': 179075, 'created_utc': 1714072077.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.923+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_simedz82', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Faster Postgres Migrations', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 73, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccwob5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/DxiMluDRad_KzKujP4qCxBXkwg1hNKC1i1ymutwvyqI.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1714063226.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'blog.peerdb.io', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://blog.peerdb.io/how-can-we-make-pgdump-and-pgrestore-5-times-faster', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/ElCFZRmu5KUysokSN3_N6ALSh0uGzkD9Fa8872l0Ogg.jpg?auto=webp&s=f6c4503b988637af51f5966857fbc73d793f5ff7', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/ElCFZRmu5KUysokSN3_N6ALSh0uGzkD9Fa8872l0Ogg.jpg?width=108&crop=smart&auto=webp&s=7979dd9a51050bbebc1b68441604445e4a012a91', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/ElCFZRmu5KUysokSN3_N6ALSh0uGzkD9Fa8872l0Ogg.jpg?width=216&crop=smart&auto=webp&s=99abbcfc21bcf54753a1d2019304442ae975cc04', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/ElCFZRmu5KUysokSN3_N6ALSh0uGzkD9Fa8872l0Ogg.jpg?width=320&crop=smart&auto=webp&s=454a86ff8cbd6e70ad0dba807e780be171b5698b', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/ElCFZRmu5KUysokSN3_N6ALSh0uGzkD9Fa8872l0Ogg.jpg?width=640&crop=smart&auto=webp&s=3e172757dee2bd10910af82014e3e8c1f1122648', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/ElCFZRmu5KUysokSN3_N6ALSh0uGzkD9Fa8872l0Ogg.jpg?width=960&crop=smart&auto=webp&s=9c2818b83920f1cf1937c8c7c7919f38ffba5e33', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/ElCFZRmu5KUysokSN3_N6ALSh0uGzkD9Fa8872l0Ogg.jpg?width=1080&crop=smart&auto=webp&s=7c9750cf58cf5f5dd2be6a1ddf8249247f73cf0c', 'width': 1080, 'height': 567}], 'variants': {}, 'id': '9fod4cB5ucrIIoRlUgXwychzwXLqKWeHitVk6GEbQQ8'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1ccwob5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='saipeerdb'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccwob5/faster_***_migrations/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://blog.peerdb.io/how-can-we-make-pgdump-and-pgrestore-5-times-faster', 'subreddit_subscribers': 179075, 'created_utc': 1714063226.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.924+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_12wozut7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "Excited to give you all a  👀 sneak peek demo into Teradata's VantageCloud Lake 🚀", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 105, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccv5as', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'ups': 1, 'total_awards_received': 0, 'media_embed': {'content': '<iframe width="356" height="200" src="https://www.youtube.com/embed/0fsm39quUhY?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Exploring VantageCloud Lake | Teradata"></iframe>', 'width': 356, 'scrolling': False, 'height': 200}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Exploring VantageCloud Lake | Teradata', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '<iframe width="356" height="200" src="https://www.youtube.com/embed/0fsm39quUhY?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Exploring VantageCloud Lake | Teradata"></iframe>', 'author_name': 'Teradata', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/0fsm39quUhY/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/@teradata'}}, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {'content': '<iframe width="356" height="200" src="https://www.youtube.com/embed/0fsm39quUhY?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Exploring VantageCloud Lake | Teradata"></iframe>', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/1ccv5as', 'height': 200}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/tXQ033Vtlq5BoeF-RDQA5AB63S8tMHz06agTUI-v9PE.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'rich:video', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1714059639.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'youtu.be', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://youtu.be/0fsm39quUhY?si=f_urSUxNVXd8x99P', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/and6imGhuhmb3TFH-5Oad7kpLYzVlhCI76gTPn_Sjdk.jpg?auto=webp&s=36ead72b48edc57e39c1a4b9844fe4678e5fd65c', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/and6imGhuhmb3TFH-5Oad7kpLYzVlhCI76gTPn_Sjdk.jpg?width=108&crop=smart&auto=webp&s=10c6d5cc862eaca616652d29c15c7abf7e64582f', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/and6imGhuhmb3TFH-5Oad7kpLYzVlhCI76gTPn_Sjdk.jpg?width=216&crop=smart&auto=webp&s=cac5e83b340111c03c8e3450a930e3afce2080d2', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/and6imGhuhmb3TFH-5Oad7kpLYzVlhCI76gTPn_Sjdk.jpg?width=320&crop=smart&auto=webp&s=2327aa0c0efca96e8186ec47ef22500b232974b2', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'jyIyTXux1CBIfvLB39-J_2nGLYLPYH4kH7u-QKo8uwU'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1ccv5as', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='JanethL'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccv5as/excited_to_give_you_all_a_sneak_peek_demo_into/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://youtu.be/0fsm39quUhY?si=f_urSUxNVXd8x99P', 'subreddit_subscribers': 179075, 'created_utc': 1714059639.0, 'num_crossposts': 0, 'media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Exploring VantageCloud Lake | Teradata', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '<iframe width="356" height="200" src="https://www.youtube.com/embed/0fsm39quUhY?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Exploring VantageCloud Lake | Teradata"></iframe>', 'author_name': 'Teradata', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/0fsm39quUhY/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/@teradata'}}, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.925+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Discover how integrating [NLP based data pipeline tool](https://askondata.com/) amplifies insights from unstructured text data. Explore NLP's role in extracting, transforming, and loading data efficiently. Discuss real-world applications, challenges, and future trends. Join us to uncover strategies for leveraging NLP to drive informed decision-making and gain competitive advantage.", 'author_fullname': 't2_cyygz69f1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Leveraging NLP in Data Pipelines for Enhanced Insights', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccqdto', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714047179.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Discover how integrating <a href="https://askondata.com/">NLP based data pipeline tool</a> amplifies insights from unstructured text data. Explore NLP&#39;s role in extracting, transforming, and loading data efficiently. Discuss real-world applications, challenges, and future trends. Join us to uncover strategies for leveraging NLP to drive informed decision-making and gain competitive advantage.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/dDkKJq3kx-MfDDRqgZSi3Zbtj7E6AsM8yvUiFHEWqxM.jpg?auto=webp&s=2531fe66bd59696cf9a18a62e55454bd02a89dea', 'width': 308, 'height': 300}, 'resolutions': [{'url': 'https://external-preview.redd.it/dDkKJq3kx-MfDDRqgZSi3Zbtj7E6AsM8yvUiFHEWqxM.jpg?width=108&crop=smart&auto=webp&s=aa14adb1a3eb7440da96139de22b9cc1da090d00', 'width': 108, 'height': 105}, {'url': 'https://external-preview.redd.it/dDkKJq3kx-MfDDRqgZSi3Zbtj7E6AsM8yvUiFHEWqxM.jpg?width=216&crop=smart&auto=webp&s=7907bc3df11d7f896987d3e4336b6ceb2a46d957', 'width': 216, 'height': 210}], 'variants': {}, 'id': 't7ntalc9V0ou_734t08LdXIo5wpJ7xttiRRCHVO_BEY'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ccqdto', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='VarshaH_1234'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccqdto/leveraging_nlp_in_data_pipelines_for_enhanced/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccqdto/leveraging_nlp_in_data_pipelines_for_enhanced/', 'subreddit_subscribers': 179075, 'created_utc': 1714047179.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.925+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi everyone, complete Newbie to data Engineering working in a junior role. What book recs do you have to master Microsoft SQL specifically. \n\nI’m currently practicing with sqlbolt, watched a bunch of YouTube videos but I think reading, might really help too. \n\nAny recs?\n\nThanks in advance ', 'author_fullname': 't2_vtml5pqn4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Noob learning resources ?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cd8bs7', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714093473.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone, complete Newbie to data Engineering working in a junior role. What book recs do you have to master Microsoft SQL specifically. </p>\n\n<p>I’m currently practicing with sqlbolt, watched a bunch of YouTube videos but I think reading, might really help too. </p>\n\n<p>Any recs?</p>\n\n<p>Thanks in advance </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1cd8bs7', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='rantedranter'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cd8bs7/noob_learning_resources/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cd8bs7/noob_learning_resources/', 'subreddit_subscribers': 179075, 'created_utc': 1714093473.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.925+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': " I'm currently a computer science and engineering major with approximately 1 year left until graduation. I'm contemplating applying for positions in data engineering, even though I lack professional experience in the field. I have a decent grasp of Python for coding, along with basic knowledge of Java. Additionally, I'm proficient in data structures and algorithms, and I have a good understanding of front-end development (HTML, CSS, JS, Angular). Furthermore, I possess basic SQL skills.\n\nConsidering the booming demand for data engineering roles, I believe it could be a promising career path for me. However, I'm seeking guidance on the necessary tools and skills to acquire to make myself marketable in this field. What specific technologies should I focus on learning? Are there any particular projects or certifications that would enhance my chances of landing a job as a data engineer?\n\nMoreover, I'm curious about the salary prospects for freshers in the data engineering domain. Can I expect a competitive salary despite my lack of professional experience?\n\nAny advice or suggestions would be greatly appreciated as I navigate this transition from academia to the professional world of data engineering.\n\nThank you all for your insights and support!", 'author_fullname': 't2_mkamh3m7h', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Seeking Suggestions: Can I Secure a Job as a Big Data Engineer as a Fresher with No Experience?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cdfiow', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.33, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714117322.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m currently a computer science and engineering major with approximately 1 year left until graduation. I&#39;m contemplating applying for positions in data engineering, even though I lack professional experience in the field. I have a decent grasp of Python for coding, along with basic knowledge of Java. Additionally, I&#39;m proficient in data structures and algorithms, and I have a good understanding of front-end development (HTML, CSS, JS, Angular). Furthermore, I possess basic SQL skills.</p>\n\n<p>Considering the booming demand for data engineering roles, I believe it could be a promising career path for me. However, I&#39;m seeking guidance on the necessary tools and skills to acquire to make myself marketable in this field. What specific technologies should I focus on learning? Are there any particular projects or certifications that would enhance my chances of landing a job as a data engineer?</p>\n\n<p>Moreover, I&#39;m curious about the salary prospects for freshers in the data engineering domain. Can I expect a competitive salary despite my lack of professional experience?</p>\n\n<p>Any advice or suggestions would be greatly appreciated as I navigate this transition from academia to the professional world of data engineering.</p>\n\n<p>Thank you all for your insights and support!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cdfiow', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ZenphyriX'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cdfiow/seeking_suggestions_can_i_secure_a_job_as_a_big/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cdfiow/seeking_suggestions_can_i_secure_a_job_as_a_big/', 'subreddit_subscribers': 179075, 'created_utc': 1714117322.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.926+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff773d8880>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Context\n\nOptimizing Operations for Lenskart\n\nBackground\n\nLenskart, a leading eyewear retailer, aims to streamline its operations by centralizing product information,\n\npricing details, and customer reviews, and enhancing customer and employee databases.\n\nProcedure Implementation\n\n1.1 Gather Product Data\n\n\uf0b7 The team accesses the Lenskart website to extract detailed information on eyeglasses, sunglasses, and\n\ncontact lenses.\n\n\uf0b7 Product pricing details, including discounts and promotions, are compiled for a comprehensive\n\noverview.\n\n\uf0b7 Customer, order-related information with employee details needs to be obtained from the client side.\n\n\n\n1.2 Platform build\n\n\uf0b7 Build an architecture for this data platform.\n\n\uf0b7 Apply data transformation techniques to cleanse and structure the extracted data.\n\n\uf0b7 Need to ingest them all into the platform and ensure data integrity and consistency during the loading\n\nprocess for accurate analytics and reporting.\n\n\n\n1.3 Document and Metadata Creation\n\n\uf0b7 Create documentation outlining the structure, relationships, and metadata of the loaded data within the\n\ndata warehouse.\n\n\uf0b7 This documentation serves as a reference guide for team members interacting with the data.\n\n\n\nDataset information\n\nScraped Dataset: (Need to scrape from the site)\n\n\uf0b7 Product\n\n\uf0b7 Store\n\nClient Dataset: (Will be shared through the Azure Blob storage)\n\n\uf0b7 Customer\n\n\uf0b7 Transaction\n\nExpecting KPI\n\n\uf0b7 LTM bridge\n\n\uf0b7 Top store, customer, and product sliced and diced in different dimensions.\n\n\uf0b7 Heat maps (e.g., sales distribution with geo-spatial data)\n\n\uf0b7 Customer LTV (Optional if we have time towards end of project)\n\nOutcome\n\n\n\n3\n\nThe streamlined procedure facilitates better coordination between different teams, leading to optimized\n\nwarehousing operations. The shared document serves as a central reference point, enabling Lenskart to\n\nrespond proactively to market trends and customer preferences. Sales and customer service teams use the\n\ncustomer details to tailor their interactions, enhancing the overall customer experience. Also, this platform needs\n\nto provide a solution for operational-related areas. Overall, Lenskart achieves operational efficiency and\n\nimproved customer satisfaction through the implementation of this comprehensive procedure.', 'author_fullname': 't2_b2s0yyvxc', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "i am beginner to data engineering i am familiar with python and sql . i am currently under the training my trainer gave the project description . but i don't know what to do next please help me what do i need to do this project.", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cdf21y', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.33, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714115395.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Context</p>\n\n<p>Optimizing Operations for Lenskart</p>\n\n<p>Background</p>\n\n<p>Lenskart, a leading eyewear retailer, aims to streamline its operations by centralizing product information,</p>\n\n<p>pricing details, and customer reviews, and enhancing customer and employee databases.</p>\n\n<p>Procedure Implementation</p>\n\n<p>1.1 Gather Product Data</p>\n\n<p>\uf0b7 The team accesses the Lenskart website to extract detailed information on eyeglasses, sunglasses, and</p>\n\n<p>contact lenses.</p>\n\n<p>\uf0b7 Product pricing details, including discounts and promotions, are compiled for a comprehensive</p>\n\n<p>overview.</p>\n\n<p>\uf0b7 Customer, order-related information with employee details needs to be obtained from the client side.</p>\n\n<p>1.2 Platform build</p>\n\n<p>\uf0b7 Build an architecture for this data platform.</p>\n\n<p>\uf0b7 Apply data transformation techniques to cleanse and structure the extracted data.</p>\n\n<p>\uf0b7 Need to ingest them all into the platform and ensure data integrity and consistency during the loading</p>\n\n<p>process for accurate analytics and reporting.</p>\n\n<p>1.3 Document and Metadata Creation</p>\n\n<p>\uf0b7 Create documentation outlining the structure, relationships, and metadata of the loaded data within the</p>\n\n<p>data warehouse.</p>\n\n<p>\uf0b7 This documentation serves as a reference guide for team members interacting with the data.</p>\n\n<p>Dataset information</p>\n\n<p>Scraped Dataset: (Need to scrape from the site)</p>\n\n<p>\uf0b7 Product</p>\n\n<p>\uf0b7 Store</p>\n\n<p>Client Dataset: (Will be shared through the Azure Blob storage)</p>\n\n<p>\uf0b7 Customer</p>\n\n<p>\uf0b7 Transaction</p>\n\n<p>Expecting KPI</p>\n\n<p>\uf0b7 LTM bridge</p>\n\n<p>\uf0b7 Top store, customer, and product sliced and diced in different dimensions.</p>\n\n<p>\uf0b7 Heat maps (e.g., sales distribution with geo-spatial data)</p>\n\n<p>\uf0b7 Customer LTV (Optional if we have time towards end of project)</p>\n\n<p>Outcome</p>\n\n<p>3</p>\n\n<p>The streamlined procedure facilitates better coordination between different teams, leading to optimized</p>\n\n<p>warehousing operations. The shared document serves as a central reference point, enabling Lenskart to</p>\n\n<p>respond proactively to market trends and customer preferences. Sales and customer service teams use the</p>\n\n<p>customer details to tailor their interactions, enhancing the overall customer experience. Also, this platform needs</p>\n\n<p>to provide a solution for operational-related areas. Overall, Lenskart achieves operational efficiency and</p>\n\n<p>improved customer satisfaction through the implementation of this comprehensive procedure.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cdf21y', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Worried-Towel-5886'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cdf21y/i_am_beginner_to_data_engineering_i_am_familiar/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cdf21y/i_am_beginner_to_data_engineering_i_am_familiar/', 'subreddit_subscribers': 179075, 'created_utc': 1714115395.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-26T12:45:14.974+0000] {python.py:194} INFO - Done. Returned value was: /opt/airflow/data/output/reddit_20240426.csv
[2024-04-26T12:45:14.989+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, execution_date=20240426T124512, start_date=20240426T124513, end_date=20240426T124514
[2024-04-26T12:45:15.016+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2024-04-26T12:45:15.026+0000] {taskinstance.py:2778} INFO - 1 downstream tasks scheduled from follow-on schedule check
