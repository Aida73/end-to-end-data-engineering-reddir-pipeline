[2024-04-25T15:21:56.335+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-04-25T15:21:54.918110+00:00 [queued]>
[2024-04-25T15:21:56.339+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-04-25T15:21:54.918110+00:00 [queued]>
[2024-04-25T15:21:56.339+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-04-25T15:21:56.344+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-04-25 15:21:54.918110+00:00
[2024-04-25T15:21:56.348+0000] {standard_task_runner.py:57} INFO - Started process 65 to run task
[2024-04-25T15:21:56.349+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'manual__2024-04-25T15:21:54.918110+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmpzasp_b5b']
[2024-04-25T15:21:56.351+0000] {standard_task_runner.py:85} INFO - Job 7: Subtask reddit_extraction
[2024-04-25T15:21:56.368+0000] {task_command.py:416} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-04-25T15:21:54.918110+00:00 [running]> on host 1bd1b115cb8b
[2024-04-25T15:21:56.406+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Aida Sow' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-04-25T15:21:54.918110+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-04-25T15:21:54.918110+00:00'
[2024-04-25T15:21:56.420+0000] {logging_mixin.py:151} INFO - connected to reddit!
[2024-04-25T15:21:57.262+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I know this is off topic but wanted to go to the source (you nerds).  \n\nI was laid off my Enterprise sales job late last year.  Have found myself wanting to jump into a role that serves data engineers for my next gig.  I have done a bit of advisory/consulting around DE topics but did not spend 100% of my time consulting in that area.\n\nCompanies like Monte Carlo Data, Red Panda, Grafana, and Cribl all look to be selling great products that move the needle in different ways.\n\nAny other products/companies I should be looking at? Want to help you all do your jobs better! ', 'author_fullname': 't2_84nodhxf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What data engineering product are you most excited to buy? Unemployed sales rep looking for the right company to work for.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cc9gh3', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 45, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 45, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1713992354.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I know this is off topic but wanted to go to the source (you nerds).  </p>\n\n<p>I was laid off my Enterprise sales job late last year.  Have found myself wanting to jump into a role that serves data engineers for my next gig.  I have done a bit of advisory/consulting around DE topics but did not spend 100% of my time consulting in that area.</p>\n\n<p>Companies like Monte Carlo Data, Red Panda, Grafana, and Cribl all look to be selling great products that move the needle in different ways.</p>\n\n<p>Any other products/companies I should be looking at? Want to help you all do your jobs better! </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cc9gh3', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='babaisfun'), 'discussion_type': None, 'num_comments': 56, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cc9gh3/what_data_engineering_product_are_you_most/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cc9gh3/what_data_engineering_product_are_you_most/', 'subreddit_subscribers': 178870, 'created_utc': 1713992354.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.264+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_vxh97cx1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Comparison of Different Stream Processing Platforms', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 121, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccodiy', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.98, 'author_flair_background_color': 'transparent', 'ups': 33, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': '287cf772-ac9d-11eb-aa84-0ead36cb44af', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 33, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/2n7TKGeaeQJmuQuTJNe0_DiAgmG-pIZAkRxpAHwFAoQ.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1714040274.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/4jqfd7fnplwc1.png', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/4jqfd7fnplwc1.png?auto=webp&s=e18564ea19571b5048539c78c6b43242825ef3be', 'width': 1730, 'height': 1496}, 'resolutions': [{'url': 'https://preview.redd.it/4jqfd7fnplwc1.png?width=108&crop=smart&auto=webp&s=15a4acf2a0ed8107b2d9058a128ee091746c6ee2', 'width': 108, 'height': 93}, {'url': 'https://preview.redd.it/4jqfd7fnplwc1.png?width=216&crop=smart&auto=webp&s=e72539a6782e0068fb0618afb4790c9abb045c6b', 'width': 216, 'height': 186}, {'url': 'https://preview.redd.it/4jqfd7fnplwc1.png?width=320&crop=smart&auto=webp&s=cdf40ffb0bc504c71ca19c9e084bb7a7b1dbd71a', 'width': 320, 'height': 276}, {'url': 'https://preview.redd.it/4jqfd7fnplwc1.png?width=640&crop=smart&auto=webp&s=b92139ab01add8b48934f5f2916b65b5f658ef07', 'width': 640, 'height': 553}, {'url': 'https://preview.redd.it/4jqfd7fnplwc1.png?width=960&crop=smart&auto=webp&s=fcb70c47a7a58811b66b2cd649f3716e752d3370', 'width': 960, 'height': 830}, {'url': 'https://preview.redd.it/4jqfd7fnplwc1.png?width=1080&crop=smart&auto=webp&s=9a7edede174d9a93b4e3d779d7df785b26ab2f9e', 'width': 1080, 'height': 933}], 'variants': {}, 'id': '5AM8ujCRwbjBlXo5K1n5-VCYdOlDunTauZcej68fSPk'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Software Engineer', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ccodiy', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='wanshao'), 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1ccodiy/comparison_of_different_stream_processing/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://i.redd.it/4jqfd7fnplwc1.png', 'subreddit_subscribers': 178870, 'created_utc': 1714040274.0, 'num_crossposts': 1, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.264+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'It seems like all companies ‚Äúright now‚Äù are in cost cutting mode. Most are trying to do the best with what they have, surgically hiring / backfilling. I‚Äôm curious hear what types of problems / project are actually getting funded. ', 'author_fullname': 't2_ez4cm3m01', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What‚Äôs one problem your organization is willing to spend money to solve?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccdveg', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.82, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 28, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 28, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714003760.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>It seems like all companies ‚Äúright now‚Äù are in cost cutting mode. Most are trying to do the best with what they have, surgically hiring / backfilling. I‚Äôm curious hear what types of problems / project are actually getting funded. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ccdveg', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='glinter777'), 'discussion_type': None, 'num_comments': 45, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccdveg/whats_one_problem_your_organization_is_willing_to/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccdveg/whats_one_problem_your_organization_is_willing_to/', 'subreddit_subscribers': 178870, 'created_utc': 1714003760.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.265+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I recently published an article on Medium discussing the layers of a reporting pipeline. I discuss the key steps in developing such a pipeline, breaking it down into distinct layers and explaining the processes that occur at each stage. \n\nI thought the folks in this sub might find it interesting. Keen to hear whether you agree or disagree with some of my thoughts.\n\n', 'author_fullname': 't2_ojr03vx2i', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'An Opinionated Guide to Layering your Analytics Pipelines ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 37, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccd6rs', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': 'transparent', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': '9ecf3c88-e787-11ed-957e-de1616aeae13', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/q2zzA9ueUcSATjWL4oKCZ4QSuMIa9b8JT5uRQfeMn8k.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1714001884.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'medium.com', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I recently published an article on Medium discussing the layers of a reporting pipeline. I discuss the key steps in developing such a pipeline, breaking it down into distinct layers and explaining the processes that occur at each stage. </p>\n\n<p>I thought the folks in this sub might find it interesting. Keen to hear whether you agree or disagree with some of my thoughts.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://medium.com/@nydas/an-opinionated-guide-to-layering-your-analytics-and-reporting-pipelines-b1d8a510b671', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/oJvWY4iy8S9khBs45tSfygR4DOybp1yTFsVQKWKVeyI.jpg?auto=webp&s=e3adf74bcc9cd49962d61c3e75c8e9aa8220f75f', 'width': 1182, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/oJvWY4iy8S9khBs45tSfygR4DOybp1yTFsVQKWKVeyI.jpg?width=108&crop=smart&auto=webp&s=9dea393659a05acfc8b3eda0b4d599de5d031723', 'width': 108, 'height': 28}, {'url': 'https://external-preview.redd.it/oJvWY4iy8S9khBs45tSfygR4DOybp1yTFsVQKWKVeyI.jpg?width=216&crop=smart&auto=webp&s=c40ef8834a5065d94eb74ff2388b852816cbc2da', 'width': 216, 'height': 57}, {'url': 'https://external-preview.redd.it/oJvWY4iy8S9khBs45tSfygR4DOybp1yTFsVQKWKVeyI.jpg?width=320&crop=smart&auto=webp&s=f4ad5f4d039bce066f536dc7825c816414cf27ed', 'width': 320, 'height': 85}, {'url': 'https://external-preview.redd.it/oJvWY4iy8S9khBs45tSfygR4DOybp1yTFsVQKWKVeyI.jpg?width=640&crop=smart&auto=webp&s=ec41a4f320fbce16cc42f537ca6c75f919222150', 'width': 640, 'height': 171}, {'url': 'https://external-preview.redd.it/oJvWY4iy8S9khBs45tSfygR4DOybp1yTFsVQKWKVeyI.jpg?width=960&crop=smart&auto=webp&s=329fc2bd139d39b327836408a5e1defb517bcbdb', 'width': 960, 'height': 256}, {'url': 'https://external-preview.redd.it/oJvWY4iy8S9khBs45tSfygR4DOybp1yTFsVQKWKVeyI.jpg?width=1080&crop=smart&auto=webp&s=57b9ad217d184b79b115e7c3b2c855b18feaef33', 'width': 1080, 'height': 288}], 'variants': {}, 'id': 'tDVPCpgpSGaAQRWCFncFGdwCTZ8MXuYv5XkRaSO9qzg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Data Engineering Manager', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1ccd6rs', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='nydasco'), 'discussion_type': None, 'num_comments': 11, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1ccd6rs/an_opinionated_guide_to_layering_your_analytics/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://medium.com/@nydas/an-opinionated-guide-to-layering-your-analytics-and-reporting-pipelines-b1d8a510b671', 'subreddit_subscribers': 178870, 'created_utc': 1714001884.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.266+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I am currently a data engineer but mainly do on-premises work with legacy infrastructure (Autosys as our scheduler, IBM tools for Ci/Cd). We move data and process them using stored procedures, Linux, and PowerShell. I think these are dead-end technologies and would like to learn cloud development to apply to other jobs. \n\nMy company is in a hybrid model, so some teams are working with cloud services. Fortunately, I have access to the tools we use: GCP BigQuery and Gcp Composer (Airflow scheduler). \n\nI‚Äôm currently dabbling with doing simple things like creating airflow DAGs on simple tasks like creating and deleting tables and copying data from one table to another. \n\nMy question is: \n\nDo you know of good projects I can do to get the concept down of these cloud tools, enough to put it on my LinkedIn? I won‚Äôt have any professional projects so I‚Äôll have to create them myself. \n\nI want to do projects so I feel confident enough to apply for jobs using these technologies. I am currently just creating tables, copying, inserting, etc all using Python and big query.\n\nWhat kind of projects should I be doing to feel like I‚Äôm getting a good grasp of real data engineering on the cloud? \n\nThank you in advance!', 'author_fullname': 't2_3r8hfu8u', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'On-Prem trying to get cloud experience. What kind of cloud projects for realistic data engineer cloud experience?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccfhl8', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714008394.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am currently a data engineer but mainly do on-premises work with legacy infrastructure (Autosys as our scheduler, IBM tools for Ci/Cd). We move data and process them using stored procedures, Linux, and PowerShell. I think these are dead-end technologies and would like to learn cloud development to apply to other jobs. </p>\n\n<p>My company is in a hybrid model, so some teams are working with cloud services. Fortunately, I have access to the tools we use: GCP BigQuery and Gcp Composer (Airflow scheduler). </p>\n\n<p>I‚Äôm currently dabbling with doing simple things like creating airflow DAGs on simple tasks like creating and deleting tables and copying data from one table to another. </p>\n\n<p>My question is: </p>\n\n<p>Do you know of good projects I can do to get the concept down of these cloud tools, enough to put it on my LinkedIn? I won‚Äôt have any professional projects so I‚Äôll have to create them myself. </p>\n\n<p>I want to do projects so I feel confident enough to apply for jobs using these technologies. I am currently just creating tables, copying, inserting, etc all using Python and big query.</p>\n\n<p>What kind of projects should I be doing to feel like I‚Äôm getting a good grasp of real data engineering on the cloud? </p>\n\n<p>Thank you in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ccfhl8', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='findingjob'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccfhl8/onprem_trying_to_get_cloud_experience_what_kind/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccfhl8/onprem_trying_to_get_cloud_experience_what_kind/', 'subreddit_subscribers': 178870, 'created_utc': 1714008394.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.267+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi, Is it common/good approach when I create/write logs with a minimum *low spec SparkApplication. Then store it in Parquet format and transfer it in to Bronze Layer?\nWhat do you think about this approach? Please let me know!', 'author_fullname': 't2_v6m6r7sq3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Parquet Format using for logs storage?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccian9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': 'a96f3daa-e787-11ed-bb3c-927138abd1d2', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714016881.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi, Is it common/good approach when I create/write logs with a minimum *low spec SparkApplication. Then store it in Parquet format and transfer it in to Bronze Layer?\nWhat do you think about this approach? Please let me know!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Junior Data Engineer', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ccian9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='sebastiandang'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1ccian9/parquet_format_using_for_logs_storage/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccian9/parquet_format_using_for_logs_storage/', 'subreddit_subscribers': 178870, 'created_utc': 1714016881.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.267+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Credit to Julien H for sharing this on Linkedin. Kinda didn't believe it at first but there you go\n\n&#x200B;\n\nhttps://preview.redd.it/j62ujlacrmwc1.png?width=1884&format=png&auto=webp&s=4f324f6092a8dab48f6bc919cc13d8bcd5b10b57\n\nLooks like people are realising our importance for Gen AI slowly, but surely.", 'author_fullname': 't2_voma7dkju', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'The Data Engineering Hype Cycle is beginning (??)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 80, 'top_awarded_type': None, 'hide_score': True, 'media_metadata': {'j62ujlacrmwc1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 62, 'x': 108, 'u': 'https://preview.redd.it/j62ujlacrmwc1.png?width=108&crop=smart&auto=webp&s=a33009471e410d729582e2123a93c8f97644d24e'}, {'y': 124, 'x': 216, 'u': 'https://preview.redd.it/j62ujlacrmwc1.png?width=216&crop=smart&auto=webp&s=831e33254d5d74365b686e1b9de469bdd791ea51'}, {'y': 184, 'x': 320, 'u': 'https://preview.redd.it/j62ujlacrmwc1.png?width=320&crop=smart&auto=webp&s=4358aab93c75e7cd9902cb82195534fcd6a58b8b'}, {'y': 369, 'x': 640, 'u': 'https://preview.redd.it/j62ujlacrmwc1.png?width=640&crop=smart&auto=webp&s=dcef815c0687285cf7f2795f483e007c338d756c'}, {'y': 553, 'x': 960, 'u': 'https://preview.redd.it/j62ujlacrmwc1.png?width=960&crop=smart&auto=webp&s=df56f37e3f1e2f472c668bebcf8b958ecce723b3'}, {'y': 623, 'x': 1080, 'u': 'https://preview.redd.it/j62ujlacrmwc1.png?width=1080&crop=smart&auto=webp&s=64832b1cd72507d4368177bd5621773e310a2347'}], 's': {'y': 1087, 'x': 1884, 'u': 'https://preview.redd.it/j62ujlacrmwc1.png?width=1884&format=png&auto=webp&s=4f324f6092a8dab48f6bc919cc13d8bcd5b10b57'}, 'id': 'j62ujlacrmwc1'}}, 'name': 't3_1ccsf39', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.77, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/KtNG9MvmMTzrlRRV0gGLYGSh3orAAmS-Q7L79zfkOcU.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714052953.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Credit to Julien H for sharing this on Linkedin. Kinda didn&#39;t believe it at first but there you go</p>\n\n<p>&#x200B;</p>\n\n<p><a href="https://preview.redd.it/j62ujlacrmwc1.png?width=1884&amp;format=png&amp;auto=webp&amp;s=4f324f6092a8dab48f6bc919cc13d8bcd5b10b57">https://preview.redd.it/j62ujlacrmwc1.png?width=1884&amp;format=png&amp;auto=webp&amp;s=4f324f6092a8dab48f6bc919cc13d8bcd5b10b57</a></p>\n\n<p>Looks like people are realising our importance for Gen AI slowly, but surely.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ccsf39', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='engineer_of-sorts'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccsf39/the_data_engineering_hype_cycle_is_beginning/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccsf39/the_data_engineering_hype_cycle_is_beginning/', 'subreddit_subscribers': 178870, 'created_utc': 1714052953.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.268+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello People, I need your advice!\n\nI need to provide data for data analysis in my company. My company uses AWS RDS MariaDB. Is it a better option to move the data with CDC to S3 as parquet files for the data lake, or just use Lake Formation to fetch the data directly from RDS MariaDB? Which option is more performant? My understanding is RDS MariaDB is optimized for OLTP workloads. If I use Lake Formation to fetch data, will it be faster than querying Parquet files from S3 using Athena or Redshift Spectrum?\n\nNote: i am talking about data size of 20 TB. The newly formed data analytics team wants real time data. A little lag is acceptable', 'author_fullname': 't2_8qydg4ea', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'S3 Datalake or just use Lake Formation', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cco6ge', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714039535.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello People, I need your advice!</p>\n\n<p>I need to provide data for data analysis in my company. My company uses AWS RDS MariaDB. Is it a better option to move the data with CDC to S3 as parquet files for the data lake, or just use Lake Formation to fetch the data directly from RDS MariaDB? Which option is more performant? My understanding is RDS MariaDB is optimized for OLTP workloads. If I use Lake Formation to fetch data, will it be faster than querying Parquet files from S3 using Athena or Redshift Spectrum?</p>\n\n<p>Note: i am talking about data size of 20 TB. The newly formed data analytics team wants real time data. A little lag is acceptable</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cco6ge', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Ok_Illustrator72'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cco6ge/s3_datalake_or_just_use_lake_formation/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cco6ge/s3_datalake_or_just_use_lake_formation/', 'subreddit_subscribers': 178870, 'created_utc': 1714039535.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.268+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi team,\n\n\xa0\n\nRequire some expert advice here from a newbie. Not solving for world hunger but looking for the best way to solve this problem and wondering how the rest of the data engineering world would go about solutioning this.\n\nDetails are:\n\n1.\xa0\xa0\xa0\xa0\xa0\xa0 My source systems are API files and the ingested format is stored in ADLS GEN2 as parquet files.\n\n2.\xa0\xa0\xa0\xa0\xa0\xa0 I use Databricks and the medallion architecture and delta tables for data engineering activities.\n\n3.\xa0\xa0\xa0\xa0\xa0\xa0 My target application is SAP Datasphere. Its SAP‚Äôs cloud solution to data warehouse. If you are wondering why not perform the data engineering activity in SAP Datasphere, we can but for the sake of this conversation, lets leave that for another day.\n\nSo the problem that I am trying to understand here is this, given that delta table in Databricks is made out of multiple parquet files and another folder call \\_delta\\_log, in my Gold layer, I am seeing multiple parquet file and this additional \\_delta\\_log folder. I know this is not an issue if I am querying the folder as opposed to the individual parquet file.\n\nSo the question here is this, when I expose my ADLS GEN2 for the GOLD layer to SAP Datasphere, SAP Datasphere will have access to all of the parquet files and \\_delta\\_log folder. I don‚Äôt believe this is an issue but I am concern that an uninformed user will read the individual parquet file instead of the entire folder and produce incorrect result.\n\nI am wondering if it is best to store my business level aggregate data in the gold layer in a database before as opposed to exposing my ADLS GEN2 to avoid this problem.\n\nI have also attached a screenshot from SAP Datasphere and as you can see, it contains multiple parquet file.\n\nhttps://preview.redd.it/s6wldo34dgwc1.png?width=881&format=png&auto=webp&s=ecb734ec15757edaab84f20e4c38c33315763e1c\n\nAm I overthinking this and any guidance is greatly appreciated.\n\n\xa0\n\n\xa0\n\n\xa0\n\n\xa0', 'author_fullname': 't2_5b8k9nl1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Should I store my gold layer data in a database? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 106, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'s6wldo34dgwc1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 82, 'x': 108, 'u': 'https://preview.redd.it/s6wldo34dgwc1.png?width=108&crop=smart&auto=webp&s=2d2ab3fb7443913817fcf70383c8083a5b6f5e29'}, {'y': 165, 'x': 216, 'u': 'https://preview.redd.it/s6wldo34dgwc1.png?width=216&crop=smart&auto=webp&s=bf5cc890a73845dee6a8c5f99684429c2009cd7e'}, {'y': 244, 'x': 320, 'u': 'https://preview.redd.it/s6wldo34dgwc1.png?width=320&crop=smart&auto=webp&s=184a5f68f72b2904073cd3212900a67e72a35538'}, {'y': 488, 'x': 640, 'u': 'https://preview.redd.it/s6wldo34dgwc1.png?width=640&crop=smart&auto=webp&s=a680dd0dfd05e5febb07d7917a23cfc0b8054587'}], 's': {'y': 673, 'x': 881, 'u': 'https://preview.redd.it/s6wldo34dgwc1.png?width=881&format=png&auto=webp&s=ecb734ec15757edaab84f20e4c38c33315763e1c'}, 'id': 's6wldo34dgwc1'}}, 'name': 't3_1cc2cl9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/O-vKg97wKJyIzNBiCTWPcZehDLW4NVWJichETx9BCTE.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1713975502.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi team,</p>\n\n<p>\xa0</p>\n\n<p>Require some expert advice here from a newbie. Not solving for world hunger but looking for the best way to solve this problem and wondering how the rest of the data engineering world would go about solutioning this.</p>\n\n<p>Details are:</p>\n\n<p>1.\xa0\xa0\xa0\xa0\xa0\xa0 My source systems are API files and the ingested format is stored in ADLS GEN2 as parquet files.</p>\n\n<p>2.\xa0\xa0\xa0\xa0\xa0\xa0 I use Databricks and the medallion architecture and delta tables for data engineering activities.</p>\n\n<p>3.\xa0\xa0\xa0\xa0\xa0\xa0 My target application is SAP Datasphere. Its SAP‚Äôs cloud solution to data warehouse. If you are wondering why not perform the data engineering activity in SAP Datasphere, we can but for the sake of this conversation, lets leave that for another day.</p>\n\n<p>So the problem that I am trying to understand here is this, given that delta table in Databricks is made out of multiple parquet files and another folder call _delta_log, in my Gold layer, I am seeing multiple parquet file and this additional _delta_log folder. I know this is not an issue if I am querying the folder as opposed to the individual parquet file.</p>\n\n<p>So the question here is this, when I expose my ADLS GEN2 for the GOLD layer to SAP Datasphere, SAP Datasphere will have access to all of the parquet files and _delta_log folder. I don‚Äôt believe this is an issue but I am concern that an uninformed user will read the individual parquet file instead of the entire folder and produce incorrect result.</p>\n\n<p>I am wondering if it is best to store my business level aggregate data in the gold layer in a database before as opposed to exposing my ADLS GEN2 to avoid this problem.</p>\n\n<p>I have also attached a screenshot from SAP Datasphere and as you can see, it contains multiple parquet file.</p>\n\n<p><a href="https://preview.redd.it/s6wldo34dgwc1.png?width=881&amp;format=png&amp;auto=webp&amp;s=ecb734ec15757edaab84f20e4c38c33315763e1c">https://preview.redd.it/s6wldo34dgwc1.png?width=881&amp;format=png&amp;auto=webp&amp;s=ecb734ec15757edaab84f20e4c38c33315763e1c</a></p>\n\n<p>Am I overthinking this and any guidance is greatly appreciated.</p>\n\n<p>\xa0</p>\n\n<p>\xa0</p>\n\n<p>\xa0</p>\n\n<p>\xa0</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cc2cl9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='scht1980'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cc2cl9/should_i_store_my_gold_layer_data_in_a_database/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cc2cl9/should_i_store_my_gold_layer_data_in_a_database/', 'subreddit_subscribers': 178870, 'created_utc': 1713975502.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.269+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "With my Movie Data Modeling Challenge officially underway, I released a blog packed with insights and proven strategies designed to help data professionals dominate not only this challenge, but any data project.\n\nAll insights are drawn from extensive discussions with top performers from my recent NBA Data Modeling Challenge. They told me what works, and I just took notes! üìù\n\nSneak peek of what you'll find in the blog:\n\n**A Well-Defined Strategy:** Master the art of setting clear objectives, formulating questions, embracing the 'measure twice, cut once' approach, and effectively telling stories with data.\n\n**Leveraging Paradime:** Learn how to maximize Paradime's robust features to enhance your analytics engineering productivity and streamline your SQL and dbt development processes. (This tool is required in the challenge)\n\nWhether you're aiming to dominate the Movie Data Modeling Challenge or seeking to refine your techniques in data projects, these insights are invaluable.\n\n[Dive into the full blog here!](https://www.paradime.io/blog/winning-strategies-movie-challenge)", 'author_fullname': 't2_nkrhcqia', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'BS-Free Guide to Dominating the Movie Data Modeling Challenge‚Äîand Beyond!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cc79rr', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1713987093.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>With my Movie Data Modeling Challenge officially underway, I released a blog packed with insights and proven strategies designed to help data professionals dominate not only this challenge, but any data project.</p>\n\n<p>All insights are drawn from extensive discussions with top performers from my recent NBA Data Modeling Challenge. They told me what works, and I just took notes! üìù</p>\n\n<p>Sneak peek of what you&#39;ll find in the blog:</p>\n\n<p><strong>A Well-Defined Strategy:</strong> Master the art of setting clear objectives, formulating questions, embracing the &#39;measure twice, cut once&#39; approach, and effectively telling stories with data.</p>\n\n<p><strong>Leveraging Paradime:</strong> Learn how to maximize Paradime&#39;s robust features to enhance your analytics engineering productivity and streamline your SQL and dbt development processes. (This tool is required in the challenge)</p>\n\n<p>Whether you&#39;re aiming to dominate the Movie Data Modeling Challenge or seeking to refine your techniques in data projects, these insights are invaluable.</p>\n\n<p><a href="https://www.paradime.io/blog/winning-strategies-movie-challenge">Dive into the full blog here!</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/f1JWiLCufyd7H2k-A6g8_0CdvBY1qS4jFOKVkht3JPU.jpg?auto=webp&s=6198c1de0723d47ad5bfd1be26cd07a3062d3195', 'width': 1280, 'height': 720}, 'resolutions': [{'url': 'https://external-preview.redd.it/f1JWiLCufyd7H2k-A6g8_0CdvBY1qS4jFOKVkht3JPU.jpg?width=108&crop=smart&auto=webp&s=e4e7c80419d3f10cbc5c00dd6fb610902a80b32e', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/f1JWiLCufyd7H2k-A6g8_0CdvBY1qS4jFOKVkht3JPU.jpg?width=216&crop=smart&auto=webp&s=d923efcadcf50797df9d77d5f834dcd22f3ccf0c', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/f1JWiLCufyd7H2k-A6g8_0CdvBY1qS4jFOKVkht3JPU.jpg?width=320&crop=smart&auto=webp&s=a98e897443e3c72f86d0429b6f22316f85826a2e', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/f1JWiLCufyd7H2k-A6g8_0CdvBY1qS4jFOKVkht3JPU.jpg?width=640&crop=smart&auto=webp&s=565482adb9a00748a6b80a54f6af958d2c5f572b', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/f1JWiLCufyd7H2k-A6g8_0CdvBY1qS4jFOKVkht3JPU.jpg?width=960&crop=smart&auto=webp&s=38723670330de9d5aa1c00dd0f616730de12baa4', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/f1JWiLCufyd7H2k-A6g8_0CdvBY1qS4jFOKVkht3JPU.jpg?width=1080&crop=smart&auto=webp&s=382e6baf70e71370512d6a5c130649f262f53ad7', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'liaFn2gfTEjJkZZnId8J8P2IMpNxhf0_Q31pacEYANI'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1cc79rr', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='JParkerRogers'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cc79rr/bsfree_guide_to_dominating_the_movie_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cc79rr/bsfree_guide_to_dominating_the_movie_data/', 'subreddit_subscribers': 178870, 'created_utc': 1713987093.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.271+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello there,\n\nI've been working on and stuck on a data modeling problem for a while.\n\nIncoming data is typical product-attribute data. Products have common attributes, but every product type has specific attributes, like PCs' RAM capacity and CPU core count, TVs' panel technology, resolution, etc. Some attributes have more than 1 value.\n\nI need to design a data model for our data lakehouse, which operates with Delta Lake. \n\nHere are the proposal models that I am considering:\n\n1)\n\nProductTable -> Every product sits here with its common attributes.\n\nPcProductTable, TVProductTable, ‚Ä¶. -> Create separate tables for each product type and keep specific attributes as separate columns\n\nMulti-valued attributes can be divided into a lookup table or kept in an array inside the corresponding table.\n\n2)\n\nProductTable -> Every product sits here with its common attributes.\n\nAttributeTable -> EAV table for keeping all attributes in a single table with a value column.\n\nMulti-valued attributes are just inserted as a new row in the table.\n\nExample data:\t\n\nproduct_id , attribute_name , value\n\n1,ram_capacity,8GB\n\n1,country,USA\n\n1,country,Canada\n\n2,resouliton,1920x1080\n\n2,energy consumption,70w\n\n\nMy main concerns are applying delta changes and keeping track of historical data. Applying SDC to the second approach sounds complex, but I don't have any clue how to prove it.\n\nCan you evaluate the proposed models in terms of dimensional modelling and reporting performance?  \nHave you guys ever designed a warehouse/lakehouse like this? \nWhat is the best approach do you suggest?\n\nI appreciate your help!", 'author_fullname': 't2_13pduy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Need help with product-attribute data modelling', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccoerb', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714040407.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello there,</p>\n\n<p>I&#39;ve been working on and stuck on a data modeling problem for a while.</p>\n\n<p>Incoming data is typical product-attribute data. Products have common attributes, but every product type has specific attributes, like PCs&#39; RAM capacity and CPU core count, TVs&#39; panel technology, resolution, etc. Some attributes have more than 1 value.</p>\n\n<p>I need to design a data model for our data lakehouse, which operates with Delta Lake. </p>\n\n<p>Here are the proposal models that I am considering:</p>\n\n<p>1)</p>\n\n<p>ProductTable -&gt; Every product sits here with its common attributes.</p>\n\n<p>PcProductTable, TVProductTable, ‚Ä¶. -&gt; Create separate tables for each product type and keep specific attributes as separate columns</p>\n\n<p>Multi-valued attributes can be divided into a lookup table or kept in an array inside the corresponding table.</p>\n\n<p>2)</p>\n\n<p>ProductTable -&gt; Every product sits here with its common attributes.</p>\n\n<p>AttributeTable -&gt; EAV table for keeping all attributes in a single table with a value column.</p>\n\n<p>Multi-valued attributes are just inserted as a new row in the table.</p>\n\n<p>Example data:   </p>\n\n<p>product_id , attribute_name , value</p>\n\n<p>1,ram_capacity,8GB</p>\n\n<p>1,country,USA</p>\n\n<p>1,country,Canada</p>\n\n<p>2,resouliton,1920x1080</p>\n\n<p>2,energy consumption,70w</p>\n\n<p>My main concerns are applying delta changes and keeping track of historical data. Applying SDC to the second approach sounds complex, but I don&#39;t have any clue how to prove it.</p>\n\n<p>Can you evaluate the proposed models in terms of dimensional modelling and reporting performance?<br/>\nHave you guys ever designed a warehouse/lakehouse like this? \nWhat is the best approach do you suggest?</p>\n\n<p>I appreciate your help!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ccoerb', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='cevadfolyok'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccoerb/need_help_with_productattribute_data_modelling/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccoerb/need_help_with_productattribute_data_modelling/', 'subreddit_subscribers': 178870, 'created_utc': 1714040407.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.271+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "In my company, the ETL is done using plain old SQL. Now that I will be given ETL post, I want to have other software or tech to do the ETL.\n\nWe are mostly on-prem Oracle Database. It's a courier & logistics company so data is in millions here.\n\nPlease if you could suggest me any software or any other alternative which would help me in my case.\n\nAnd just to let you know, I'm coming from a BI background, I've been doing SQL, Power BI and some Python for last 2 years (if this info is also relevant)", 'author_fullname': 't2_3k9gevl', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'New ETL software suggestion, switching from SQL ETL', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cck5fd', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714023247.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>In my company, the ETL is done using plain old SQL. Now that I will be given ETL post, I want to have other software or tech to do the ETL.</p>\n\n<p>We are mostly on-prem Oracle Database. It&#39;s a courier &amp; logistics company so data is in millions here.</p>\n\n<p>Please if you could suggest me any software or any other alternative which would help me in my case.</p>\n\n<p>And just to let you know, I&#39;m coming from a BI background, I&#39;ve been doing SQL, Power BI and some Python for last 2 years (if this info is also relevant)</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cck5fd', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Pillstyr'), 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cck5fd/new_etl_software_suggestion_switching_from_sql_etl/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cck5fd/new_etl_software_suggestion_switching_from_sql_etl/', 'subreddit_subscribers': 178870, 'created_utc': 1714023247.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.272+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello everyone, currently, I'm dealing with a situation where data is moved to a silver layer, and external tables are created on top of it. Unfortunately, there are instances where additional columns are added to the source data, causing our external SQL databases to break.\n\nOur current workaround involves manually dropping and recreating the external table, which does the job, and Synapse successfully detects the datatypes. However, we aim to automate this process.\n\nOne workaround I've considered is running a notebook after the pipeline, which drops and recreates the table to ensure the schema is up to date. Additionally, we might be able to compare the number of columns between the silver layer and the external SQL database later on to see if we can run this when it needs to. \n\nThe only challenge is ensuring that Synapse autodetects column types like it does when done manually. I'm not entirely sure how to achieve this.\n\n&#x200B;\n\nAny advice is appreciated\n\n&#x200B;", 'author_fullname': 't2_sihhnnlo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Schema Evolution with Serverless SQL databases', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cc12yr', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1713972485.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello everyone, currently, I&#39;m dealing with a situation where data is moved to a silver layer, and external tables are created on top of it. Unfortunately, there are instances where additional columns are added to the source data, causing our external SQL databases to break.</p>\n\n<p>Our current workaround involves manually dropping and recreating the external table, which does the job, and Synapse successfully detects the datatypes. However, we aim to automate this process.</p>\n\n<p>One workaround I&#39;ve considered is running a notebook after the pipeline, which drops and recreates the table to ensure the schema is up to date. Additionally, we might be able to compare the number of columns between the silver layer and the external SQL database later on to see if we can run this when it needs to. </p>\n\n<p>The only challenge is ensuring that Synapse autodetects column types like it does when done manually. I&#39;m not entirely sure how to achieve this.</p>\n\n<p>&#x200B;</p>\n\n<p>Any advice is appreciated</p>\n\n<p>&#x200B;</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cc12yr', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Mathlete7'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cc12yr/schema_evolution_with_serverless_sql_databases/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cc12yr/schema_evolution_with_serverless_sql_databases/', 'subreddit_subscribers': 178870, 'created_utc': 1713972485.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.273+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi all,\n\nI‚Äôm a data engineer and I‚Äôve been learning the ropes in cloud since starting one and a bit years ago. I moved to a company that advertised predominantly cloud tech with a tiny bit of on-prem, but so far for the third sprint, we‚Äôre consumed by on-prem work that I‚Äôm not trained in and I‚Äôm starting to get fed up.\n\nI‚Äôm doing some certifications outside of work for cloud but at the same time considering changing companies as my career trajectory is cloud and not on-prem. Has anyone been in this position? Is there any point in staying too long?\n\nThanks ', 'author_fullname': 't2_bs6bpgld', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Job expectation vs reality', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccrag8', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714049923.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all,</p>\n\n<p>I‚Äôm a data engineer and I‚Äôve been learning the ropes in cloud since starting one and a bit years ago. I moved to a company that advertised predominantly cloud tech with a tiny bit of on-prem, but so far for the third sprint, we‚Äôre consumed by on-prem work that I‚Äôm not trained in and I‚Äôm starting to get fed up.</p>\n\n<p>I‚Äôm doing some certifications outside of work for cloud but at the same time considering changing companies as my career trajectory is cloud and not on-prem. Has anyone been in this position? Is there any point in staying too long?</p>\n\n<p>Thanks </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1ccrag8', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='JackalTheFulgid'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccrag8/job_expectation_vs_reality/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccrag8/job_expectation_vs_reality/', 'subreddit_subscribers': 178870, 'created_utc': 1714049923.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.274+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm working with a database with HR data, and I'm gonna create a tabular model. Most of the tables have start- and end dates.\nSome of the tables have dimensional information like about where employees work at different time periods.\n\nSo my question is what will be the best solution? To create one big table with 1 date for all employees, or use measures between the tables using their start- and end dates?\n\nResources like articles/videos about this subject would also be appreciated.\n", 'author_fullname': 't2_asugk82m', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Multiple Fact Tables vs One', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cco7qf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714039676.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m working with a database with HR data, and I&#39;m gonna create a tabular model. Most of the tables have start- and end dates.\nSome of the tables have dimensional information like about where employees work at different time periods.</p>\n\n<p>So my question is what will be the best solution? To create one big table with 1 date for all employees, or use measures between the tables using their start- and end dates?</p>\n\n<p>Resources like articles/videos about this subject would also be appreciated.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cco7qf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Due-Quality1498'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cco7qf/multiple_fact_tables_vs_one/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cco7qf/multiple_fact_tables_vs_one/', 'subreddit_subscribers': 178870, 'created_utc': 1714039676.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.274+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I am new to the ETL development field. I have worked on ETL testing before but I wanted to try my hands on development. I want to learn ETL strategies, datawarehouse design techniques, data modeling. Any resources are welcome, thanks in advance!', 'author_fullname': 't2_66o9yyui', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'ETL, Data Modeling and Database design ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cclunj', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714029785.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am new to the ETL development field. I have worked on ETL testing before but I wanted to try my hands on development. I want to learn ETL strategies, datawarehouse design techniques, data modeling. Any resources are welcome, thanks in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cclunj', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Siddboss195803'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cclunj/etl_data_modeling_and_database_design/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cclunj/etl_data_modeling_and_database_design/', 'subreddit_subscribers': 178870, 'created_utc': 1714029785.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.275+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi, data engineers - I'm new to Apache Beam and I'm working out how it might apply to an unusual use case...\n\nLet's say I'd like to run a pipeline that will send exactly 10,000 email alerts (or as many as it can before running out of data), with the addresses to attempt stored in a BigQuery table. Sensibly, you would do this by just querying for 10,000 rows so your input PCollection had the correct number, and push them all through the pipeline.\n\nThe hangup is that in this situation, I can't tell whether an email can count as successful until after it's attempted to process. If processing didn't happen (because the email was on a blocklist, etc) I'd like to keep going until the maximum of 10,000 has been fulfilled.\n\nTo do this I think I need to do a couple of things:\n\n* Keep track of the number of successful processes across all the workers running the job. If we've reached the limit, process no more rows.\n* Pull more rows as needed until the limit is fulfilled (does some sort of streaming arrangement exist in Beam for BigQuery so that a query can pull more rows until the limit has been fulfilled?)\n\nIs Beam a suitable solution for this kind of arrangement, or am I thinking about it wrongly? Thanks for any advice!", 'author_fullname': 't2_ufwr5pri', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Apache Beam - Pipeline unlimited input, limited output?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccf7nm', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714007589.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi, data engineers - I&#39;m new to Apache Beam and I&#39;m working out how it might apply to an unusual use case...</p>\n\n<p>Let&#39;s say I&#39;d like to run a pipeline that will send exactly 10,000 email alerts (or as many as it can before running out of data), with the addresses to attempt stored in a BigQuery table. Sensibly, you would do this by just querying for 10,000 rows so your input PCollection had the correct number, and push them all through the pipeline.</p>\n\n<p>The hangup is that in this situation, I can&#39;t tell whether an email can count as successful until after it&#39;s attempted to process. If processing didn&#39;t happen (because the email was on a blocklist, etc) I&#39;d like to keep going until the maximum of 10,000 has been fulfilled.</p>\n\n<p>To do this I think I need to do a couple of things:</p>\n\n<ul>\n<li>Keep track of the number of successful processes across all the workers running the job. If we&#39;ve reached the limit, process no more rows.</li>\n<li>Pull more rows as needed until the limit is fulfilled (does some sort of streaming arrangement exist in Beam for BigQuery so that a query can pull more rows until the limit has been fulfilled?)</li>\n</ul>\n\n<p>Is Beam a suitable solution for this kind of arrangement, or am I thinking about it wrongly? Thanks for any advice!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ccf7nm', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='colour_doesnt_matter'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccf7nm/apache_beam_pipeline_unlimited_input_limited/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccf7nm/apache_beam_pipeline_unlimited_input_limited/', 'subreddit_subscribers': 178870, 'created_utc': 1714007589.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.275+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'As title states, right now struggling to load power bi efficiently. Having to use the ODBC connection rather than Postgres since I have SSL enabled. I think what‚Äôs happened is it has forced everything into a single thread.\n\nThe Postgres database is on premise running on a Linux server. \n\nOptions on the table right now:\n1. Turn SSL off for connections coming from on network machines with PowerBI desktop\n2. Use PowerBI gateway\n3. Direct Query\n4. ???\n\nOur CISO is skeptical of PowerBI gateway so anyone working that angle, I‚Äôd love to hear how it‚Äôs not a security risk, etc.', 'author_fullname': 't2_ao7u40a', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Postgres > PowerBI loading recommendations ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccau5j', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1713995737.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>As title states, right now struggling to load power bi efficiently. Having to use the ODBC connection rather than Postgres since I have SSL enabled. I think what‚Äôs happened is it has forced everything into a single thread.</p>\n\n<p>The Postgres database is on premise running on a Linux server. </p>\n\n<p>Options on the table right now:\n1. Turn SSL off for connections coming from on network machines with PowerBI desktop\n2. Use PowerBI gateway\n3. Direct Query\n4. ???</p>\n\n<p>Our CISO is skeptical of PowerBI gateway so anyone working that angle, I‚Äôd love to hear how it‚Äôs not a security risk, etc.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ccau5j', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='minormisgnomer'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccau5j/***_powerbi_loading_recommendations/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccau5j/***_powerbi_loading_recommendations/', 'subreddit_subscribers': 178870, 'created_utc': 1713995737.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.276+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I am querying the source table with a filter greater than the last\\_update\\_time. My source (update) df has 940 distinct (deduped) rows (Databricks). I am merging into the target table (delta format) with when matched on the key, update set \\* and when not matched insert \\*. My target table does not have duplicates. 633 rows are matching. When I look at the Operation Metrics (in Databricks) of the target table on the "merge" operation, I see that 633 rows have been matched and updated, and 374 rows have been inserted, and the source df rows are 940. But 633 + 374 = 1007. Shouldn\'t my updated and inserted rows sum up to 940? What are those extra 67 rows? ', 'author_fullname': 't2_wk1jb3h2k', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Delta format merge into question', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccaaih', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1713994380.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am querying the source table with a filter greater than the last_update_time. My source (update) df has 940 distinct (deduped) rows (Databricks). I am merging into the target table (delta format) with when matched on the key, update set * and when not matched insert *. My target table does not have duplicates. 633 rows are matching. When I look at the Operation Metrics (in Databricks) of the target table on the &quot;merge&quot; operation, I see that 633 rows have been matched and updated, and 374 rows have been inserted, and the source df rows are 940. But 633 + 374 = 1007. Shouldn&#39;t my updated and inserted rows sum up to 940? What are those extra 67 rows? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ccaaih', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='DataDarvesh'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccaaih/delta_format_merge_into_question/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccaaih/delta_format_merge_into_question/', 'subreddit_subscribers': 178870, 'created_utc': 1713994380.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.277+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I‚Äôve got a use case where I have a table of ‚Äúconfigurations‚Äù by ID and another table that holds the base data. The configurations table has an ID along with a string column which is a WHERE clause. My objective is to produce one table with the ID plus the results of a query based on the configuration.\n\nConfig Table \n\n|ID|CONFIG|\n|:-|:-|\n|ABC123|(region=‚ÄòA‚Äô and segment in (‚Äòs1‚Äô,s2‚Äô))|\n|||\n\nBase Data Table \n\n\n\n|Region|Segment|Customer Type|\n|:-|:-|:-|\n|A|S1|T1|\n|B|S1|T9|\n\n\n\nWhen we did this in Snowflake and DBT we used a Jinja loop to build a SQL statement comprised of UNION statements for each ID. Now that we have thousands of ID values we are nearing the upper limit for the size of a single SQL statement/script. Now we want to port this to Postgres for a semi unrelated reason.\n\nIs porting this over to a Stored Proc that would be called for each ID the *only* solution here? Obviously performance is going to be a big factor, but I am struggling to come up with an alternative solution for the problem of dynamic SQL queries.\n\nTIA!', 'author_fullname': 't2_52cbaf2f', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Dynamic SQL in Postgres', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cc3zjf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1713979316.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I‚Äôve got a use case where I have a table of ‚Äúconfigurations‚Äù by ID and another table that holds the base data. The configurations table has an ID along with a string column which is a WHERE clause. My objective is to produce one table with the ID plus the results of a query based on the configuration.</p>\n\n<p>Config Table </p>\n\n<table><thead>\n<tr>\n<th align="left">ID</th>\n<th align="left">CONFIG</th>\n</tr>\n</thead><tbody>\n<tr>\n<td align="left">ABC123</td>\n<td align="left">(region=‚ÄòA‚Äô and segment in (‚Äòs1‚Äô,s2‚Äô))</td>\n</tr>\n<tr>\n<td align="left"></td>\n<td align="left"></td>\n</tr>\n</tbody></table>\n\n<p>Base Data Table </p>\n\n<table><thead>\n<tr>\n<th align="left">Region</th>\n<th align="left">Segment</th>\n<th align="left">Customer Type</th>\n</tr>\n</thead><tbody>\n<tr>\n<td align="left">A</td>\n<td align="left">S1</td>\n<td align="left">T1</td>\n</tr>\n<tr>\n<td align="left">B</td>\n<td align="left">S1</td>\n<td align="left">T9</td>\n</tr>\n</tbody></table>\n\n<p>When we did this in Snowflake and DBT we used a Jinja loop to build a SQL statement comprised of UNION statements for each ID. Now that we have thousands of ID values we are nearing the upper limit for the size of a single SQL statement/script. Now we want to port this to Postgres for a semi unrelated reason.</p>\n\n<p>Is porting this over to a Stored Proc that would be called for each ID the <em>only</em> solution here? Obviously performance is going to be a big factor, but I am struggling to come up with an alternative solution for the problem of dynamic SQL queries.</p>\n\n<p>TIA!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cc3zjf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='yoquierodata'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cc3zjf/dynamic_sql_in_***/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cc3zjf/dynamic_sql_in_***/', 'subreddit_subscribers': 178870, 'created_utc': 1713979316.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.277+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi folks, sysadmin in a tiny enterprise here, today wearing a data engineer hat :)  \nwas pitted with a task of creating a data warehouse on prem, for BI purposes. C-suite wants some performance and financial data from different departments nicely displayed in series of different dashboards.  \nthe source of data coming in will be couple of local sql db instances(3-4) I do not expect a major amount of data, mainly sales figures and some performance metrics. As cost is a major factor the whole stack has to be opensource. Did a bit of googling and came up with stack as follows:  \nApache Airflow for connection to sql db(source data) postgreSQL as DB, dbt for modelling and Redash for dashboards.  \nDoes the above setup makes sense from the requirement point?   \nI will be the sole implementer and maintainer of this platform so ideally for me would be to have a stack build out of ready made programs, rather than going the Python route and developing some components myself(lack the coding skill and time, my proficiency in Python = being able to edit the code that chatGPT spits out so it somewhat does what I need:)   \nappreciate any advice on this, thanks.', 'author_fullname': 't2_nc7je1zy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Opensource solution for a tiny data warehouse.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1ccs3vs', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714052117.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi folks, sysadmin in a tiny enterprise here, today wearing a data engineer hat :)<br/>\nwas pitted with a task of creating a data warehouse on prem, for BI purposes. C-suite wants some performance and financial data from different departments nicely displayed in series of different dashboards.<br/>\nthe source of data coming in will be couple of local sql db instances(3-4) I do not expect a major amount of data, mainly sales figures and some performance metrics. As cost is a major factor the whole stack has to be opensource. Did a bit of googling and came up with stack as follows:<br/>\nApache Airflow for connection to sql db(source data) postgreSQL as DB, dbt for modelling and Redash for dashboards.<br/>\nDoes the above setup makes sense from the requirement point?<br/>\nI will be the sole implementer and maintainer of this platform so ideally for me would be to have a stack build out of ready made programs, rather than going the Python route and developing some components myself(lack the coding skill and time, my proficiency in Python = being able to edit the code that chatGPT spits out so it somewhat does what I need:)<br/>\nappreciate any advice on this, thanks.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ccs3vs', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='LeatherPuzzled3855'), 'discussion_type': None, 'num_comments': 11, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccs3vs/opensource_solution_for_a_tiny_data_warehouse/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccs3vs/opensource_solution_for_a_tiny_data_warehouse/', 'subreddit_subscribers': 178870, 'created_utc': 1714052117.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.278+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Tl;dr: db name error when deploying even after fixing it in code\n\nSomeone üåö added a non-conforming database name in terraform. So when we run terraform plan we got the error: error reading glue catalog database... Unexpected format... Expected CATALOG-ID:DATABASE-NAME \n\nThe issue seems to come from a database name that is stored in a glue module. He üåö probably copy and pasted the bucket url with "s3://" in the db name field. \n\nQuickfix right? I\'d think so, but every name mention has been fixed whoever terraform doesn\'t deploy and throw the same error. I tried to rollback the changes in git and the issue continues. \n\nAny clue or insight is welcome. \nPlz help', 'author_fullname': 't2_5etps', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Non conforming database name in aws using terraform', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccpqff', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714045136.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Tl;dr: db name error when deploying even after fixing it in code</p>\n\n<p>Someone üåö added a non-conforming database name in terraform. So when we run terraform plan we got the error: error reading glue catalog database... Unexpected format... Expected CATALOG-ID:DATABASE-NAME </p>\n\n<p>The issue seems to come from a database name that is stored in a glue module. He üåö probably copy and pasted the bucket url with &quot;s3://&quot; in the db name field. </p>\n\n<p>Quickfix right? I&#39;d think so, but every name mention has been fixed whoever terraform doesn&#39;t deploy and throw the same error. I tried to rollback the changes in git and the issue continues. </p>\n\n<p>Any clue or insight is welcome. \nPlz help</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ccpqff', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='brunudumal'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccpqff/non_conforming_database_name_in_aws_using/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccpqff/non_conforming_database_name_in_aws_using/', 'subreddit_subscribers': 178870, 'created_utc': 1714045136.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.278+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I follow the https://devblogs.microsoft.com/azure-sql/azure-sql-change-stream-with-debezium/\n\nWho explain how to install and configurate this solution. But I'm stuck. When I install the container all the config eventhubs were created( config,status and offset) but when I create my connector with the api command between the Azure sqlserver and Debezium none topic or eventhubs was created in azure.\n\nIf you have an idea?  It could be awesome. \nDo you think is up to date solution or I should spend my time to create my own CDC message producer?", 'author_fullname': 't2_srer0x4c', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'CDC with Azure SQL server + Debezium + eventhub ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccla6n', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714027604.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I follow the <a href="https://devblogs.microsoft.com/azure-sql/azure-sql-change-stream-with-debezium/">https://devblogs.microsoft.com/azure-sql/azure-sql-change-stream-with-debezium/</a></p>\n\n<p>Who explain how to install and configurate this solution. But I&#39;m stuck. When I install the container all the config eventhubs were created( config,status and offset) but when I create my connector with the api command between the Azure sqlserver and Debezium none topic or eventhubs was created in azure.</p>\n\n<p>If you have an idea?  It could be awesome. \nDo you think is up to date solution or I should spend my time to create my own CDC message producer?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/ffqHxDhMZV-JohaWqjB5EumcTk8ADr4JED_aOg9Uk5Y.jpg?auto=webp&s=6ad56179d3ba774f1361e5774da2f0c0f25b1fff', 'width': 1280, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/ffqHxDhMZV-JohaWqjB5EumcTk8ADr4JED_aOg9Uk5Y.jpg?width=108&crop=smart&auto=webp&s=8cad0a6a869e42be3601ffa4413721856be14121', 'width': 108, 'height': 67}, {'url': 'https://external-preview.redd.it/ffqHxDhMZV-JohaWqjB5EumcTk8ADr4JED_aOg9Uk5Y.jpg?width=216&crop=smart&auto=webp&s=28094d3a3c0a04c5bae96baeac295df09a224fa1', 'width': 216, 'height': 135}, {'url': 'https://external-preview.redd.it/ffqHxDhMZV-JohaWqjB5EumcTk8ADr4JED_aOg9Uk5Y.jpg?width=320&crop=smart&auto=webp&s=a43a7636b90474178c9b07a3ae48d7726ee88a96', 'width': 320, 'height': 200}, {'url': 'https://external-preview.redd.it/ffqHxDhMZV-JohaWqjB5EumcTk8ADr4JED_aOg9Uk5Y.jpg?width=640&crop=smart&auto=webp&s=6cca7b133d416366fbd1a32f2e33abb37939e381', 'width': 640, 'height': 400}, {'url': 'https://external-preview.redd.it/ffqHxDhMZV-JohaWqjB5EumcTk8ADr4JED_aOg9Uk5Y.jpg?width=960&crop=smart&auto=webp&s=65bd0f3723a062f1046ba79c1e360d9a66024098', 'width': 960, 'height': 600}, {'url': 'https://external-preview.redd.it/ffqHxDhMZV-JohaWqjB5EumcTk8ADr4JED_aOg9Uk5Y.jpg?width=1080&crop=smart&auto=webp&s=f4ce88603e9f5b2ccee2b21068350d04118dcd6f', 'width': 1080, 'height': 675}], 'variants': {}, 'id': 'ex1gQ8hvWqzkv1_KFvvRvbYIx15oTd4D17I0oTEmcJ8'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ccla6n', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Legitimate-Cry-2492'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccla6n/cdc_with_azure_sql_server_debezium_eventhub/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccla6n/cdc_with_azure_sql_server_debezium_eventhub/', 'subreddit_subscribers': 178870, 'created_utc': 1714027604.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.279+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Probably  a noob question but I can't find a reference in any doc that will satisfy me of how it works..\n\nTrying to replay CDC.. inserts, updates, deletes. I've separated the inserts from mods to run inserts first so a modified record always exists in the delta table. Ok assuming keys will never be reused. Sorted both these dfs appropriately.all good.\n\nBut when I run the merge for the mods is there a guarantee they'll merge in the sort order? What if there's a bunch of updates to the same record, in rapid succession? Do i need to do anything like dedup first? Obviously I want the most recent state to be reflected at the end.", 'author_fullname': 't2_10kwn7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Delta lake merge question ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccjshm', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1714022152.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714021943.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Probably  a noob question but I can&#39;t find a reference in any doc that will satisfy me of how it works..</p>\n\n<p>Trying to replay CDC.. inserts, updates, deletes. I&#39;ve separated the inserts from mods to run inserts first so a modified record always exists in the delta table. Ok assuming keys will never be reused. Sorted both these dfs appropriately.all good.</p>\n\n<p>But when I run the merge for the mods is there a guarantee they&#39;ll merge in the sort order? What if there&#39;s a bunch of updates to the same record, in rapid succession? Do i need to do anything like dedup first? Obviously I want the most recent state to be reflected at the end.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ccjshm', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='bcsamsquanch'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccjshm/delta_lake_merge_question/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccjshm/delta_lake_merge_question/', 'subreddit_subscribers': 178870, 'created_utc': 1714021943.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.279+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi All,\nI am an experienced ETL developer with 4 years of experience in Ab Initio. Due to some circumstances, I had to work mainly on SQL and pandas only for last 2 years and lost touch with Ab Initio. Now I feel like I have to start from the scratch. Also as companies are moving away from costly tools like Ab Initio and Informatica and the trend changed due to modern data lake architecture‚Ä¶  What would be the one enterprise level ETL tool that you will recommend for learning to build data pipelines in 2024 at least for doing the EL in data integration. \n\nThanks!', 'author_fullname': 't2_sb703vnv', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Enterprise ETL Tool recommendation. ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cchtjk', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1714017365.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714015324.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi All,\nI am an experienced ETL developer with 4 years of experience in Ab Initio. Due to some circumstances, I had to work mainly on SQL and pandas only for last 2 years and lost touch with Ab Initio. Now I feel like I have to start from the scratch. Also as companies are moving away from costly tools like Ab Initio and Informatica and the trend changed due to modern data lake architecture‚Ä¶  What would be the one enterprise level ETL tool that you will recommend for learning to build data pipelines in 2024 at least for doing the EL in data integration. </p>\n\n<p>Thanks!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cchtjk', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='sidy66'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cchtjk/enterprise_etl_tool_recommendation/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cchtjk/enterprise_etl_tool_recommendation/', 'subreddit_subscribers': 178870, 'created_utc': 1714015324.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.280+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey everyone,\n\nI‚Äôm seeking some advice and resources on data migrations as I transition into a potential data engineering role. My first project might involve migrating data, possibly from legacy systems to the cloud (AWS or Snowflake). While I don‚Äôt have all the details yet, I want to be fully prepared for this task.\n\nAre there any recommended books, courses, or resources that cover the essentials of data migrations? I‚Äôm particularly interested in learning about the staple steps, writing test cases, and ensuring a smooth transition of pipelines into the new destination.\n\nIt seems like there‚Äôs a wealth of resources in the data world, but I‚Äôve found that information on data migrations is somewhat lacking. Any advice or pointers would be greatly appreciated. Thanks in advance!', 'author_fullname': 't2_malvjkjw', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Resources for Data Migrations', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cc9hjf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1713992425.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey everyone,</p>\n\n<p>I‚Äôm seeking some advice and resources on data migrations as I transition into a potential data engineering role. My first project might involve migrating data, possibly from legacy systems to the cloud (AWS or Snowflake). While I don‚Äôt have all the details yet, I want to be fully prepared for this task.</p>\n\n<p>Are there any recommended books, courses, or resources that cover the essentials of data migrations? I‚Äôm particularly interested in learning about the staple steps, writing test cases, and ensuring a smooth transition of pipelines into the new destination.</p>\n\n<p>It seems like there‚Äôs a wealth of resources in the data world, but I‚Äôve found that information on data migrations is somewhat lacking. Any advice or pointers would be greatly appreciated. Thanks in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cc9hjf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='thisisnice96'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cc9hjf/resources_for_data_migrations/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cc9hjf/resources_for_data_migrations/', 'subreddit_subscribers': 178870, 'created_utc': 1713992425.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.280+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'My understanding of these terms is that a Frontend DE is dealing with the visualisation part(Building Reports, Dashboards, etc.) and the Backend DE is dealing with preparing the data to be visualised.\n\nBut are these 2 roles actually separated or a data engineer is supposed to know and do both? \n\nI am lacking on the Frontend part, and I am not really enjoying building SSRS reports and PowerBI dashboards.\n\nI want to understand if I should focus on it, or if I can live happily in my Backend world.\n\n', 'author_fullname': 't2_4lcvdsdn', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Frontend vs Backend', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cc83af', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.8, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1713989052.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>My understanding of these terms is that a Frontend DE is dealing with the visualisation part(Building Reports, Dashboards, etc.) and the Backend DE is dealing with preparing the data to be visualised.</p>\n\n<p>But are these 2 roles actually separated or a data engineer is supposed to know and do both? </p>\n\n<p>I am lacking on the Frontend part, and I am not really enjoying building SSRS reports and PowerBI dashboards.</p>\n\n<p>I want to understand if I should focus on it, or if I can live happily in my Backend world.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cc83af', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Emotional_Key'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cc83af/frontend_vs_backend/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cc83af/frontend_vs_backend/', 'subreddit_subscribers': 178870, 'created_utc': 1713989052.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.280+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_pwrpmf1s', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Google Search Parameters (2024 Guide)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 93, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cc5l23', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/wB2OmXCZGYsGYAhTFpAZ5Rtee2Cb91buQILK5tehY_E.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1713983022.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'serpapi.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://serpapi.com/blog/google-search-parameters/', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/Dv0VSPZCcCn0iiS4JzyS9Q1WPOTOMMrRuODoROibY6w.jpg?auto=webp&s=39879007b1445a8b7832d84eba2fc1e7b6dd8c65', 'width': 1200, 'height': 798}, 'resolutions': [{'url': 'https://external-preview.redd.it/Dv0VSPZCcCn0iiS4JzyS9Q1WPOTOMMrRuODoROibY6w.jpg?width=108&crop=smart&auto=webp&s=77f0b980b77f9d14fd62399c4e3c79be6ae2e2d9', 'width': 108, 'height': 71}, {'url': 'https://external-preview.redd.it/Dv0VSPZCcCn0iiS4JzyS9Q1WPOTOMMrRuODoROibY6w.jpg?width=216&crop=smart&auto=webp&s=848ec68284529c8b284ba68bfb24d3efab37e53e', 'width': 216, 'height': 143}, {'url': 'https://external-preview.redd.it/Dv0VSPZCcCn0iiS4JzyS9Q1WPOTOMMrRuODoROibY6w.jpg?width=320&crop=smart&auto=webp&s=38a2df8a15d1542f41382fe17a84ed4558ab85a4', 'width': 320, 'height': 212}, {'url': 'https://external-preview.redd.it/Dv0VSPZCcCn0iiS4JzyS9Q1WPOTOMMrRuODoROibY6w.jpg?width=640&crop=smart&auto=webp&s=f4a2632cc440640814c9f62c7cfc287a4240323b', 'width': 640, 'height': 425}, {'url': 'https://external-preview.redd.it/Dv0VSPZCcCn0iiS4JzyS9Q1WPOTOMMrRuODoROibY6w.jpg?width=960&crop=smart&auto=webp&s=6982d3ef7fe1bf9a5510c921ed5866bc3e22d1a7', 'width': 960, 'height': 638}, {'url': 'https://external-preview.redd.it/Dv0VSPZCcCn0iiS4JzyS9Q1WPOTOMMrRuODoROibY6w.jpg?width=1080&crop=smart&auto=webp&s=653f9c0f768678eabd2ac3e7728cdee221799ade', 'width': 1080, 'height': 718}], 'variants': {}, 'id': 'x901KDXkj0QHyRx0bn_TGTYnDsKpkZDmQw8yc-brzqo'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1cc5l23', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='softcrater'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cc5l23/google_search_parameters_2024_guide/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://serpapi.com/blog/google-search-parameters/', 'subreddit_subscribers': 178870, 'created_utc': 1713983022.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.281+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I have about 20 TB of trading data on cryptocurrency exchanges. The data is stored on one server at Clickhouse. Every day new data comes into the database.\n\nHost spec:\nNetwork: 1 Gbit\nhdd: 2 x TOSHIBA_MG08ACA16TEY\nRAM: 128 GB\n\nThe data is used by a team of researchers and occasionally the data transfer speed is not enough if several people access the server or read and write at the same time.\n\nWhat is the best way to increase the data transfer rate in my case? Let's discuss it.", 'author_fullname': 't2_a4p093fl', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Scale trading data storage', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cc3eej', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1713977968.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have about 20 TB of trading data on cryptocurrency exchanges. The data is stored on one server at Clickhouse. Every day new data comes into the database.</p>\n\n<p>Host spec:\nNetwork: 1 Gbit\nhdd: 2 x TOSHIBA_MG08ACA16TEY\nRAM: 128 GB</p>\n\n<p>The data is used by a team of researchers and occasionally the data transfer speed is not enough if several people access the server or read and write at the same time.</p>\n\n<p>What is the best way to increase the data transfer rate in my case? Let&#39;s discuss it.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cc3eej', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='SpinachStrange9976'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cc3eej/scale_trading_data_storage/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cc3eej/scale_trading_data_storage/', 'subreddit_subscribers': 178870, 'created_utc': 1713977968.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.281+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Using airflow for the first time‚Ä¶ \nI am working on a project to test a data source integration with my warehouse. I want to take some tables from the operational DB, do some transformation and load the data in my clickhouse db. \nI am new to this so i was just selecting a table in one task and trying to convert it into a dataframe in the following task but there was information sharing error. I know the solution i just wanted to know what are some best practices to extract data transform it and then load? Best way to do data sharing between tasks etc. \n Do these three steps in three tasks or create sub-tasks for each smaller tasks and make DAGs for each process‚Ä¶ ', 'author_fullname': 't2_4p33upbp', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Airflow ETL processes', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cc0mqb', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1713971372.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Using airflow for the first time‚Ä¶ \nI am working on a project to test a data source integration with my warehouse. I want to take some tables from the operational DB, do some transformation and load the data in my clickhouse db. \nI am new to this so i was just selecting a table in one task and trying to convert it into a dataframe in the following task but there was information sharing error. I know the solution i just wanted to know what are some best practices to extract data transform it and then load? Best way to do data sharing between tasks etc. \n Do these three steps in three tasks or create sub-tasks for each smaller tasks and make DAGs for each process‚Ä¶ </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cc0mqb', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Beautiful-Law7386'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cc0mqb/airflow_etl_processes/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cc0mqb/airflow_etl_processes/', 'subreddit_subscribers': 178870, 'created_utc': 1713971372.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.282+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Preferred ways of transforming your JSON data, preferred tools for querying JSON, etc.', 'author_fullname': 't2_lnwagoki', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Tips on Dealing with JSON Data', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1ccsms9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714053512.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Preferred ways of transforming your JSON data, preferred tools for querying JSON, etc.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ccsms9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='AMDataLake'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccsms9/tips_on_dealing_with_json_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccsms9/tips_on_dealing_with_json_data/', 'subreddit_subscribers': 178870, 'created_utc': 1714053512.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.282+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Discover how integrating [NLP based data pipeline tool](https://askondata.com/) amplifies insights from unstructured text data. Explore NLP's role in extracting, transforming, and loading data efficiently. Discuss real-world applications, challenges, and future trends. Join us to uncover strategies for leveraging NLP to drive informed decision-making and gain competitive advantage.", 'author_fullname': 't2_cyygz69f1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Leveraging NLP in Data Pipelines for Enhanced Insights', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ccqdto', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714047179.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Discover how integrating <a href="https://askondata.com/">NLP based data pipeline tool</a> amplifies insights from unstructured text data. Explore NLP&#39;s role in extracting, transforming, and loading data efficiently. Discuss real-world applications, challenges, and future trends. Join us to uncover strategies for leveraging NLP to drive informed decision-making and gain competitive advantage.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/dDkKJq3kx-MfDDRqgZSi3Zbtj7E6AsM8yvUiFHEWqxM.jpg?auto=webp&s=2531fe66bd59696cf9a18a62e55454bd02a89dea', 'width': 308, 'height': 300}, 'resolutions': [{'url': 'https://external-preview.redd.it/dDkKJq3kx-MfDDRqgZSi3Zbtj7E6AsM8yvUiFHEWqxM.jpg?width=108&crop=smart&auto=webp&s=aa14adb1a3eb7440da96139de22b9cc1da090d00', 'width': 108, 'height': 105}, {'url': 'https://external-preview.redd.it/dDkKJq3kx-MfDDRqgZSi3Zbtj7E6AsM8yvUiFHEWqxM.jpg?width=216&crop=smart&auto=webp&s=7907bc3df11d7f896987d3e4336b6ceb2a46d957', 'width': 216, 'height': 210}], 'variants': {}, 'id': 't7ntalc9V0ou_734t08LdXIo5wpJ7xttiRRCHVO_BEY'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ccqdto', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='VarshaH_1234'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ccqdto/leveraging_nlp_in_data_pipelines_for_enhanced/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ccqdto/leveraging_nlp_in_data_pipelines_for_enhanced/', 'subreddit_subscribers': 178870, 'created_utc': 1714047179.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.283+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I‚Äôve created a Raycast preset for SQL to boost your database work. It can be used on Raycast AI if you have latest version.\n\nSystem instructions:\n\n    Act as a SQL expert. Your answers should include an explanation, SQL language using good practices and highlighting concepts.\n    These are the rules:\n    - Create efficient SQL queries that do not overload the server.\n    - Optimize performance through adjustments to indexes and database structures.\n    - Develop stored procedures and functions to handle complex operations.\n    - Implement SQL scripts that automate routine tasks.\n    - Manage transactions to maintain data integrity.\n    - Debug queries and scripts to fix errors.\n    - Integrate SQL with other programming languages to facilitate the creation of more robust applications.\n\n[Raycast link](https://presets.ray.so/shared?preset=%7B%22creativity%22:%22low%22,%22web_search%22:true,%22model%22:%22openai-gpt-4-turbo%22,%22name%22:%22SQL%20expert%22,%22instructions%22:%22Act%20as%20a%20SQL%20expert.%20Your%20answers%20should%20include%20an%20explanation,%20SQL%20language%20using%20good%20practices%20and%20highlighting%20concepts.%5Cn%5CnThese%20are%20the%20rules:%5Cn-%20Create%20efficient%20SQL%20queries%20that%20do%20not%20overload%20the%20server.%5Cn-%20Optimize%20performance%20through%20adjustments%20to%20indexes%20and%20database%20structures.%5Cn-%20Develop%20stored%20procedures%20and%20functions%20to%20handle%20complex%20operations.%5Cn-%20Implement%20SQL%20scripts%20that%20automate%20routine%20tasks.%5Cn-%20Manage%20transactions%20to%20maintain%20data%20integrity.%5Cn-%20Debug%20queries%20and%20scripts%20to%20fix%20errors.%5Cn-%20Integrate%20SQL%20with%20other%20programming%20languages%20to%20facilitate%20the%20creation%20of%20more%20robust%20applications.%22,%22image_generation%22:true%7D)', 'author_fullname': 't2_7pooh732j', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'SQL preset for Raycast AI', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cccjwy', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714000183.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I‚Äôve created a Raycast preset for SQL to boost your database work. It can be used on Raycast AI if you have latest version.</p>\n\n<p>System instructions:</p>\n\n<pre><code>Act as a SQL expert. Your answers should include an explanation, SQL language using good practices and highlighting concepts.\nThese are the rules:\n- Create efficient SQL queries that do not overload the server.\n- Optimize performance through adjustments to indexes and database structures.\n- Develop stored procedures and functions to handle complex operations.\n- Implement SQL scripts that automate routine tasks.\n- Manage transactions to maintain data integrity.\n- Debug queries and scripts to fix errors.\n- Integrate SQL with other programming languages to facilitate the creation of more robust applications.\n</code></pre>\n\n<p><a href="https://presets.ray.so/shared?preset=%7B%22creativity%22:%22low%22,%22web_search%22:true,%22model%22:%22openai-gpt-4-turbo%22,%22name%22:%22SQL%20expert%22,%22instructions%22:%22Act%20as%20a%20SQL%20expert.%20Your%20answers%20should%20include%20an%20explanation,%20SQL%20language%20using%20good%20practices%20and%20highlighting%20concepts.%5Cn%5CnThese%20are%20the%20rules:%5Cn-%20Create%20efficient%20SQL%20queries%20that%20do%20not%20overload%20the%20server.%5Cn-%20Optimize%20performance%20through%20adjustments%20to%20indexes%20and%20database%20structures.%5Cn-%20Develop%20stored%20procedures%20and%20functions%20to%20handle%20complex%20operations.%5Cn-%20Implement%20SQL%20scripts%20that%20automate%20routine%20tasks.%5Cn-%20Manage%20transactions%20to%20maintain%20data%20integrity.%5Cn-%20Debug%20queries%20and%20scripts%20to%20fix%20errors.%5Cn-%20Integrate%20SQL%20with%20other%20programming%20languages%20to%20facilitate%20the%20creation%20of%20more%20robust%20applications.%22,%22image_generation%22:true%7D">Raycast link</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/CUfqKe-Ko9123xX3VvhqnA6S4jSrQ1iaSWeq1WNt_-w.jpg?auto=webp&s=2761b9efb37f450059e5d426573d1820f4a31e6d', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/CUfqKe-Ko9123xX3VvhqnA6S4jSrQ1iaSWeq1WNt_-w.jpg?width=108&crop=smart&auto=webp&s=295a16e1e08f9f51a8408ef161edc86ac806456c', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/CUfqKe-Ko9123xX3VvhqnA6S4jSrQ1iaSWeq1WNt_-w.jpg?width=216&crop=smart&auto=webp&s=4ee73766b6bbf58ddd9686155a53ce0ac49181c6', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/CUfqKe-Ko9123xX3VvhqnA6S4jSrQ1iaSWeq1WNt_-w.jpg?width=320&crop=smart&auto=webp&s=dc2a6a8f59b53176eaa4a9525ce4d29e9e80ef26', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/CUfqKe-Ko9123xX3VvhqnA6S4jSrQ1iaSWeq1WNt_-w.jpg?width=640&crop=smart&auto=webp&s=494f46f1259a392d114bee8a84bf9b3618ca567e', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/CUfqKe-Ko9123xX3VvhqnA6S4jSrQ1iaSWeq1WNt_-w.jpg?width=960&crop=smart&auto=webp&s=ab259670afc54b81d764dd4375b4bd2dfc452923', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/CUfqKe-Ko9123xX3VvhqnA6S4jSrQ1iaSWeq1WNt_-w.jpg?width=1080&crop=smart&auto=webp&s=201e9bc3d63aeb4f0de3ff632171873cb0fb0a21', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'XqjPOSsqAH6v4oZ8RXodAEjurn1PCTRWB3aRzkKche4'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1cccjwy', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='0xIgnacio'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cccjwy/sql_preset_for_raycast_ai/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cccjwy/sql_preset_for_raycast_ai/', 'subreddit_subscribers': 178870, 'created_utc': 1714000183.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.283+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I've got like 3 projects that require building or troubleshooting custom airbyte connectors.  I'm having a heck of a time.  If somebody has either mastered the UI Builder or worked with the CDK enough to be pretty comfortable with developing in it, hit me up and I'll pay you for a couple hours of assistance/mentoring.\n\nNot looking for somebody to do it for me, but rather just do a couple pairing sessions and see if I can get unstuck on a couple things.  And I don't expect somebody to do it for free. ", 'author_fullname': 't2_156kui', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Airbyte guru wanted', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cca25z', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1713993814.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;ve got like 3 projects that require building or troubleshooting custom airbyte connectors.  I&#39;m having a heck of a time.  If somebody has either mastered the UI Builder or worked with the CDK enough to be pretty comfortable with developing in it, hit me up and I&#39;ll pay you for a couple hours of assistance/mentoring.</p>\n\n<p>Not looking for somebody to do it for me, but rather just do a couple pairing sessions and see if I can get unstuck on a couple things.  And I don&#39;t expect somebody to do it for free. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cca25z', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='reelznfeelz'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cca25z/airbyte_guru_wanted/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cca25z/airbyte_guru_wanted/', 'subreddit_subscribers': 178870, 'created_utc': 1713993814.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.283+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi guys I recently started testing out airflow and I want to know if there is an easy way to handle all the dags with github. I only came across answers where you have one repo, but that's not what I want. I want to be flexible in my workflow where I can have different projects running in on airflow instance. \n\nDo you know of any good tips or trick, lmk!", 'author_fullname': 't2_371bzl95', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'managing dags with airflow', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cc89t4', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1713989482.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi guys I recently started testing out airflow and I want to know if there is an easy way to handle all the dags with github. I only came across answers where you have one repo, but that&#39;s not what I want. I want to be flexible in my workflow where I can have different projects running in on airflow instance. </p>\n\n<p>Do you know of any good tips or trick, lmk!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1cc89t4', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='nelzon421'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cc89t4/managing_dags_with_airflow/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cc89t4/managing_dags_with_airflow/', 'subreddit_subscribers': 178870, 'created_utc': 1713989482.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.284+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Are you leveraging open source SQL databases in your projects?\n\nCheck out the article here to see the options out there: [https://www.datacoves.com/post/open-source-databases](https://www.datacoves.com/post/open-source-databases)\n\nAny experiences or questions about integrating these technologies into your tech stack would be appreciated! ', 'author_fullname': 't2_fosm1pwyy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Open Source SQL Databases - OLTP and OLAP Options', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cc6luv', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1713985475.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Are you leveraging open source SQL databases in your projects?</p>\n\n<p>Check out the article here to see the options out there: <a href="https://www.datacoves.com/post/open-source-databases">https://www.datacoves.com/post/open-source-databases</a></p>\n\n<p>Any experiences or questions about integrating these technologies into your tech stack would be appreciated! </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/TGnIf7ldUWH5PwXoJME_nUDpDaDvECKDRxOUyhm6a64.jpg?auto=webp&s=7ae6915add4eaa4648a31e15fe38fdc6761c3718', 'width': 1200, 'height': 627}, 'resolutions': [{'url': 'https://external-preview.redd.it/TGnIf7ldUWH5PwXoJME_nUDpDaDvECKDRxOUyhm6a64.jpg?width=108&crop=smart&auto=webp&s=c374fcfcbd51e53177cef38e825fc2b1c0762623', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/TGnIf7ldUWH5PwXoJME_nUDpDaDvECKDRxOUyhm6a64.jpg?width=216&crop=smart&auto=webp&s=90b6eb84d0d9a3cd1891370ff7dba0e069b3ac58', 'width': 216, 'height': 112}, {'url': 'https://external-preview.redd.it/TGnIf7ldUWH5PwXoJME_nUDpDaDvECKDRxOUyhm6a64.jpg?width=320&crop=smart&auto=webp&s=1ddd802ebe0e84bc5ce7de9bdf5a8095f179a4ed', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/TGnIf7ldUWH5PwXoJME_nUDpDaDvECKDRxOUyhm6a64.jpg?width=640&crop=smart&auto=webp&s=b25816dea75fe388fc8586afea4c12d39816cfac', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/TGnIf7ldUWH5PwXoJME_nUDpDaDvECKDRxOUyhm6a64.jpg?width=960&crop=smart&auto=webp&s=5d37b043a2099bde99819fac0880c41af0082506', 'width': 960, 'height': 501}, {'url': 'https://external-preview.redd.it/TGnIf7ldUWH5PwXoJME_nUDpDaDvECKDRxOUyhm6a64.jpg?width=1080&crop=smart&auto=webp&s=f66f80e38b09db284853840b69beacc25ba54b31', 'width': 1080, 'height': 564}], 'variants': {}, 'id': 'clQG1S4jYLeGMQGPiJaJea135spKzTvpjCTW4NIhLik'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1cc6luv', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Data-Queen-Mayra'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cc6luv/open_source_sql_databases_oltp_and_olap_options/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cc6luv/open_source_sql_databases_oltp_and_olap_options/', 'subreddit_subscribers': 178870, 'created_utc': 1713985475.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.284+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff87dc1c10>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Calling all Data Engineers!:\n\nI'm in the process of setting up a table in my AWS RDS, which serves as a crucial data source for my BI tool. As part of the ETL process, I'm consolidating data from multiple tables into a single materialized view, then transforming it into a table (prod\\_table\\_temp), dropping the existing prod\\_table, and finally renaming prod\\_table\\_temp to prod\\_table.\n\nHowever, I'm aware this approach has its drawbacks. Is there a more efficient way to handle this process, considering our current data store is AWS RDS?\n\nLooking forward to your insights", 'author_fullname': 't2_vek8k543', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Seeking expert advice for a Data Project conundrum', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cc5mfc', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1713983113.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Calling all Data Engineers!:</p>\n\n<p>I&#39;m in the process of setting up a table in my AWS RDS, which serves as a crucial data source for my BI tool. As part of the ETL process, I&#39;m consolidating data from multiple tables into a single materialized view, then transforming it into a table (prod_table_temp), dropping the existing prod_table, and finally renaming prod_table_temp to prod_table.</p>\n\n<p>However, I&#39;m aware this approach has its drawbacks. Is there a more efficient way to handle this process, considering our current data store is AWS RDS?</p>\n\n<p>Looking forward to your insights</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cc5mfc', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Sorry-Concentrate580'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cc5mfc/seeking_expert_advice_for_a_data_project_conundrum/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cc5mfc/seeking_expert_advice_for_a_data_project_conundrum/', 'subreddit_subscribers': 178870, 'created_utc': 1713983113.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-25T15:21:57.314+0000] {python.py:194} INFO - Done. Returned value was: /opt/airflow/data/output/reddit_20240425.csv
[2024-04-25T15:21:57.334+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, execution_date=20240425T152154, start_date=20240425T152156, end_date=20240425T152157
[2024-04-25T15:21:57.357+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2024-04-25T15:21:57.378+0000] {taskinstance.py:2778} INFO - 1 downstream tasks scheduled from follow-on schedule check
